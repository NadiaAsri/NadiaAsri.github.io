{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nama : Nadia Asri NIM : 180411100063 Kelas : Penambangan Data D","title":"index"},{"location":"Decision Tree/","text":"DECISION TREES \u00b6 Classification \u00b6 Decision tree membangun model klasifikasi atau regresi dalam bentuk struktur pohon. Ini memecah dataset menjadi himpunan bagian yang lebih kecil dan lebih kecil sementara pada saat yang sama pohon keputusan terkait dikembangkan secara bertahap. Hasil akhirnya adalah pohon dengan simpul keputusan dan simpul daun. Node keputusan (mis., Outlook) memiliki dua atau lebih cabang (mis., Sunny, Overcast, dan Rainy). Node daun (mis., Play) mewakili klasifikasi atau keputusan. Node keputusan teratas dalam pohon yang sesuai dengan prediktor terbaik disebut simpul akar. Pohon keputusan dapat menangani data kategorikal dan numerik. Decision Tree terdiri dari: \u00b6 Node: Tes untuk nilai atribut tertentu. Edges / Branch Sesuai dengan hasil tes dan terhubung ke simpul atau daun berikutnya. Leaf Node: Node terminal yang memprediksi hasil (mewakili label kelas atau distribusi kelas). Algorithm \u00b6 Algoritma inti untuk membangun pohon keputusan yang disebut ID3 oleh J. R. Quinlan yang menggunakan pencarian serakah top-down melalui ruang cabang yang mungkin tanpa backtracking. ID3 menggunakan Entropi dan Penguatan Informasi untuk membangun pohon keputusan. Dalam model ZeroR tidak ada prediktor, dalam model OneR kami mencoba menemukan prediktor tunggal terbaik, Bayesian naif mencakup semua prediktor yang menggunakan aturan Bayes dan asumsi independensi antara prediktor, tetapi pohon keputusan mencakup semua prediktor dengan asumsi ketergantungan antara prediktor. Entropy \u00b6 Pohon keputusan dibangun dari atas ke bawah dari simpul akar dan melibatkan mempartisi data ke dalam himpunan bagian yang berisi instance dengan nilai yang sama (homogen). Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi rata, entropinya satu. $$ Entropy(S) = \\sum_{i=1}^n {-P_i\\log_2{P_i}} $$ Keterangan : S = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Pi = Probability dari Pi terhadap P Information Gain \u00b6 Gain informasi didasarkan pada penurunan entropi setelah dataset dibagi pada atribut. Membangun pohon keputusan adalah tentang menemukan atribut yang mengembalikan perolehan informasi tertinggi (mis., Cabang yang paling homogen). $$ Gain(S,A) = entropy(S)-\\sum_{i=1}^n \\frac{|s_i|}{|s|}\\quad x \\quad entropy(S_i) $$ Keterangan : T = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Si = Probability dari Si terhadap S Data Berikut untuk menghitung Entropy,Gain yang ada di tiap data tersebut menggunakan hitugan manual : \u00b6 Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Kita harus menghitung Entropy terlebih dahulu dengan menggunakan rumus Entropy(S) $$ P_i = C0 = , P(C0)=P_1 = \\frac{10}{20} $$ $$ P_i = C0 = , P(C1)=P_2 = \\frac{10}{20} $$ $$ Entropy(S) = {-\\frac{10}{20}\\log_2{\\frac{10}{20}}}{-\\frac{10}{20}\\log_2{\\frac{10}{20}}}= 1 $$ Sudah dicari Entropynya = 1 maka selanjutnya mencari tiap Gainnya Gain dari Gender dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari CarType dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari Shirt Size dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value $$ Entropy(Male) = {-\\frac{6}{10}\\log_2{\\frac{6}{10}}}{-\\frac{4}{10}\\log_2{\\frac{4}{10}}}=0,97051 $$ $$ Entropy(Female) = {-\\frac{4}{10}\\log_2{\\frac{4}{10}}}{-\\frac{6}{10}\\log_2{\\frac{6}{10}}}=0,97051 $$ $$ Entropy(Family) = {-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}}=0,81128 $$ $$ Entropy(Sports) = {-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}=0 $$ $$ Entropy(Luxury) = {-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}=0,54356 $$ $$ Gain(S,CarType) = 1-(\\frac{4}{20}X({-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}})+\\frac{8}{20}X({-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}+\\frac{8}{20}X({-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}) = 0,62032 $$ $$ Entropy(Small) = {-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}}=0,97095 $$ $$ Entropy(Medium) = {-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}=0,98523 $$ $$ Entropy(Large) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Entropy(ExtraLarge) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Gain(S,ShirtSize) = 1-(\\frac{5}{20}X({-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}})+\\frac{7}{20}X({-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}) = 0,012433 $$ Program untuk menghitung entropy, gain dengan program: import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.tree import DecisionTreeClassifier from sklearn import tree data = pd . read_excel ( \"data_informationgain.xlsx\" ) df = pd . DataFrame ( data ) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Merubah label / kolom yang ada di data diatas untuk bisa dihitung dengan menggunakan code python,yaitu untuk memudahkan pengguna agar bisa menghitung code yang nanti akan digunakan . Pada gender hanya menggunakan biner (0,1){F,M}. Pada cartype menggunakan inisialisasi angka urutan (0,1,2){Familiy,Luxury,Sports} . Pada Shirt Size menggunakan inisialisasi angka urutan (0,1,2,3){Small,Medium,Large,ExtraLarge}.Berikut codenya lab = LabelEncoder () df [ \"gender_n\" ] = lab . fit_transform ( df [ \"Gender\" ]) df [ \"car_type_n\" ] = lab . fit_transform ( df [ \"Car Type\" ]) df [ \"shirt_size_n\" ] = lab . fit_transform ( df [ \"Shirt Size\" ]) df [ \"class_n\" ] = lab . fit_transform ( df [ \"Class\" ]) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class gender_n car_type_n shirt_size_n class_n 1 M Family Small C0 1 0 3 0 2 M Sports Medium C0 1 2 2 0 3 M Sports Medium C0 1 2 2 0 4 M Sports Large C0 1 2 1 0 5 M Sports Extra Large C0 1 2 0 0 6 M Sports Extra Large C0 1 2 0 0 7 F Sports Small C0 0 2 3 0 8 F Sports Small C0 0 2 3 0 9 F Sports Medium C0 0 2 2 0 10 F Luxury Large C0 0 1 1 0 11 M Family Large C1 1 0 1 1 12 M Family Extra Large C1 1 0 0 1 13 M Family Medium C1 1 0 2 1 14 M Luxury Extra Large C1 1 1 0 1 15 F Luxury Small C1 0 1 3 1 16 F Luxury Small C1 0 1 3 1 17 F Luxury Medium C1 0 1 2 1 18 F Luxury Medium C1 0 1 2 1 19 F Luxury Medium C1 0 1 2 1 20 F Luxury Large C1 0 1 Setelah merubah label menjadi inisialisasi angka (numerik) melanjutkan ke code berikutnya untuk bisa dihitung dan code berikut untuk penghapusan fitur /label yang tidak diperlukan inputs = df . drop ([ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" , \"Class\" , \"class_n\" ], axis = \"columns\" ) target = df [ \"class_n\" ] Membuat Klasifikasi fitur untuk bisa dibuat Pohon Keputusan (Decision Tree)untuk bisa di classifier . model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best') from matplotlib import pyplot as plt tree . plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" ], class_names = [ \"C0\" , \"C1\" ], label = \"all\" , filled = True , impurity = True , node_ids = True , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"DECISION TREES"},{"location":"Decision Tree/#decision-trees","text":"","title":"DECISION TREES"},{"location":"Decision Tree/#classification","text":"Decision tree membangun model klasifikasi atau regresi dalam bentuk struktur pohon. Ini memecah dataset menjadi himpunan bagian yang lebih kecil dan lebih kecil sementara pada saat yang sama pohon keputusan terkait dikembangkan secara bertahap. Hasil akhirnya adalah pohon dengan simpul keputusan dan simpul daun. Node keputusan (mis., Outlook) memiliki dua atau lebih cabang (mis., Sunny, Overcast, dan Rainy). Node daun (mis., Play) mewakili klasifikasi atau keputusan. Node keputusan teratas dalam pohon yang sesuai dengan prediktor terbaik disebut simpul akar. Pohon keputusan dapat menangani data kategorikal dan numerik.","title":"Classification"},{"location":"Decision Tree/#decision-tree-terdiri-dari","text":"Node: Tes untuk nilai atribut tertentu. Edges / Branch Sesuai dengan hasil tes dan terhubung ke simpul atau daun berikutnya. Leaf Node: Node terminal yang memprediksi hasil (mewakili label kelas atau distribusi kelas).","title":"Decision Tree terdiri dari:"},{"location":"Decision Tree/#algorithm","text":"Algoritma inti untuk membangun pohon keputusan yang disebut ID3 oleh J. R. Quinlan yang menggunakan pencarian serakah top-down melalui ruang cabang yang mungkin tanpa backtracking. ID3 menggunakan Entropi dan Penguatan Informasi untuk membangun pohon keputusan. Dalam model ZeroR tidak ada prediktor, dalam model OneR kami mencoba menemukan prediktor tunggal terbaik, Bayesian naif mencakup semua prediktor yang menggunakan aturan Bayes dan asumsi independensi antara prediktor, tetapi pohon keputusan mencakup semua prediktor dengan asumsi ketergantungan antara prediktor.","title":"Algorithm"},{"location":"Decision Tree/#entropy","text":"Pohon keputusan dibangun dari atas ke bawah dari simpul akar dan melibatkan mempartisi data ke dalam himpunan bagian yang berisi instance dengan nilai yang sama (homogen). Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi rata, entropinya satu. $$ Entropy(S) = \\sum_{i=1}^n {-P_i\\log_2{P_i}} $$ Keterangan : S = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Pi = Probability dari Pi terhadap P","title":"Entropy"},{"location":"Decision Tree/#information-gain","text":"Gain informasi didasarkan pada penurunan entropi setelah dataset dibagi pada atribut. Membangun pohon keputusan adalah tentang menemukan atribut yang mengembalikan perolehan informasi tertinggi (mis., Cabang yang paling homogen). $$ Gain(S,A) = entropy(S)-\\sum_{i=1}^n \\frac{|s_i|}{|s|}\\quad x \\quad entropy(S_i) $$ Keterangan : T = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Si = Probability dari Si terhadap S","title":"Information Gain"},{"location":"Decision Tree/#data-berikut-untuk-menghitung-entropygain-yang-ada-di-tiap-data-tersebut-menggunakan-hitugan-manual","text":"Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Kita harus menghitung Entropy terlebih dahulu dengan menggunakan rumus Entropy(S) $$ P_i = C0 = , P(C0)=P_1 = \\frac{10}{20} $$ $$ P_i = C0 = , P(C1)=P_2 = \\frac{10}{20} $$ $$ Entropy(S) = {-\\frac{10}{20}\\log_2{\\frac{10}{20}}}{-\\frac{10}{20}\\log_2{\\frac{10}{20}}}= 1 $$ Sudah dicari Entropynya = 1 maka selanjutnya mencari tiap Gainnya Gain dari Gender dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari CarType dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari Shirt Size dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value $$ Entropy(Male) = {-\\frac{6}{10}\\log_2{\\frac{6}{10}}}{-\\frac{4}{10}\\log_2{\\frac{4}{10}}}=0,97051 $$ $$ Entropy(Female) = {-\\frac{4}{10}\\log_2{\\frac{4}{10}}}{-\\frac{6}{10}\\log_2{\\frac{6}{10}}}=0,97051 $$ $$ Entropy(Family) = {-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}}=0,81128 $$ $$ Entropy(Sports) = {-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}=0 $$ $$ Entropy(Luxury) = {-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}=0,54356 $$ $$ Gain(S,CarType) = 1-(\\frac{4}{20}X({-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}})+\\frac{8}{20}X({-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}+\\frac{8}{20}X({-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}) = 0,62032 $$ $$ Entropy(Small) = {-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}}=0,97095 $$ $$ Entropy(Medium) = {-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}=0,98523 $$ $$ Entropy(Large) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Entropy(ExtraLarge) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Gain(S,ShirtSize) = 1-(\\frac{5}{20}X({-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}})+\\frac{7}{20}X({-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}) = 0,012433 $$ Program untuk menghitung entropy, gain dengan program: import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.tree import DecisionTreeClassifier from sklearn import tree data = pd . read_excel ( \"data_informationgain.xlsx\" ) df = pd . DataFrame ( data ) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Merubah label / kolom yang ada di data diatas untuk bisa dihitung dengan menggunakan code python,yaitu untuk memudahkan pengguna agar bisa menghitung code yang nanti akan digunakan . Pada gender hanya menggunakan biner (0,1){F,M}. Pada cartype menggunakan inisialisasi angka urutan (0,1,2){Familiy,Luxury,Sports} . Pada Shirt Size menggunakan inisialisasi angka urutan (0,1,2,3){Small,Medium,Large,ExtraLarge}.Berikut codenya lab = LabelEncoder () df [ \"gender_n\" ] = lab . fit_transform ( df [ \"Gender\" ]) df [ \"car_type_n\" ] = lab . fit_transform ( df [ \"Car Type\" ]) df [ \"shirt_size_n\" ] = lab . fit_transform ( df [ \"Shirt Size\" ]) df [ \"class_n\" ] = lab . fit_transform ( df [ \"Class\" ]) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class gender_n car_type_n shirt_size_n class_n 1 M Family Small C0 1 0 3 0 2 M Sports Medium C0 1 2 2 0 3 M Sports Medium C0 1 2 2 0 4 M Sports Large C0 1 2 1 0 5 M Sports Extra Large C0 1 2 0 0 6 M Sports Extra Large C0 1 2 0 0 7 F Sports Small C0 0 2 3 0 8 F Sports Small C0 0 2 3 0 9 F Sports Medium C0 0 2 2 0 10 F Luxury Large C0 0 1 1 0 11 M Family Large C1 1 0 1 1 12 M Family Extra Large C1 1 0 0 1 13 M Family Medium C1 1 0 2 1 14 M Luxury Extra Large C1 1 1 0 1 15 F Luxury Small C1 0 1 3 1 16 F Luxury Small C1 0 1 3 1 17 F Luxury Medium C1 0 1 2 1 18 F Luxury Medium C1 0 1 2 1 19 F Luxury Medium C1 0 1 2 1 20 F Luxury Large C1 0 1 Setelah merubah label menjadi inisialisasi angka (numerik) melanjutkan ke code berikutnya untuk bisa dihitung dan code berikut untuk penghapusan fitur /label yang tidak diperlukan inputs = df . drop ([ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" , \"Class\" , \"class_n\" ], axis = \"columns\" ) target = df [ \"class_n\" ] Membuat Klasifikasi fitur untuk bisa dibuat Pohon Keputusan (Decision Tree)untuk bisa di classifier . model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best') from matplotlib import pyplot as plt tree . plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" ], class_names = [ \"C0\" , \"C1\" ], label = \"all\" , filled = True , impurity = True , node_ids = True , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Data Berikut untuk menghitung Entropy,Gain yang ada di tiap data tersebut menggunakan hitugan manual :"},{"location":"Missing Value/","text":"Mencari Data Hilang Menggunakan K-Nearest Neighbour \u00b6 salah satu permasalahan yang sering terjadi adalah adanya data yang hilang dalam suatu database. Metode yang paling mudah untuk menyelesaikan masalah ini adalah menggunakan Knn atau K-Nearest Neighbour. Kelemahan metode ini adalah pemilihan nilai k yang tidak tepat dapat menurunkan kinerja klasifikasi. Metode ini juga merupakan metode yang sederhana dan fleksibel dikarenakan dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit. langkah penting dalam knn adalah menentukan nilai k dimana sejumlah k tetangga terdekat dari dataset akan dijadikan sebagai nilai estimator. Pemilihan sejumlah tetangga didasarkan pada jarak salah satu instance dengan seluruh instance yang ada pada dataset. Dari informasi tetangga tersebut diperoleh estimasi nilai yang kemudian digunakan sebagai nilai imputasi pada data yang hilang. Perhitungan dalam menentukan nilai imputasi tergantung pada jenis data, untuk data kontinu digunakan rata-rata dari tetangga terdekat, sedangkan untuk data kualitatif nilai imputasi diambil dari nilai yang seringkali keluar. Algoritma KNN \u00b6 Tentukan nilai k Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan rata-rata k tetangga terdekat di Dc # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 95 , 70 , np . nan , 90 ], 'Second Score' : [ 75 , 70 , 91 , np . nan ], 'Third Score' :[ np . nan , 75 , 85 , 90 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); First Score Second Score Third Score 0 95.0 75.0 0.0 1 70.0 70.0 75.0 2 0.0 91.0 85.0 3 90.0 0.0 90.0","title":"Mencari Data Hilang Menggunakan K-Nearest Neighbour"},{"location":"Missing Value/#mencari-data-hilang-menggunakan-k-nearest-neighbour","text":"salah satu permasalahan yang sering terjadi adalah adanya data yang hilang dalam suatu database. Metode yang paling mudah untuk menyelesaikan masalah ini adalah menggunakan Knn atau K-Nearest Neighbour. Kelemahan metode ini adalah pemilihan nilai k yang tidak tepat dapat menurunkan kinerja klasifikasi. Metode ini juga merupakan metode yang sederhana dan fleksibel dikarenakan dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit. langkah penting dalam knn adalah menentukan nilai k dimana sejumlah k tetangga terdekat dari dataset akan dijadikan sebagai nilai estimator. Pemilihan sejumlah tetangga didasarkan pada jarak salah satu instance dengan seluruh instance yang ada pada dataset. Dari informasi tetangga tersebut diperoleh estimasi nilai yang kemudian digunakan sebagai nilai imputasi pada data yang hilang. Perhitungan dalam menentukan nilai imputasi tergantung pada jenis data, untuk data kontinu digunakan rata-rata dari tetangga terdekat, sedangkan untuk data kualitatif nilai imputasi diambil dari nilai yang seringkali keluar.","title":"Mencari Data Hilang Menggunakan K-Nearest Neighbour"},{"location":"Missing Value/#algoritma-knn","text":"Tentukan nilai k Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan rata-rata k tetangga terdekat di Dc # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 95 , 70 , np . nan , 90 ], 'Second Score' : [ 75 , 70 , 91 , np . nan ], 'Third Score' :[ np . nan , 75 , 85 , 90 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); First Score Second Score Third Score 0 95.0 75.0 0.0 1 70.0 70.0 75.0 2 0.0 91.0 85.0 3 90.0 0.0 90.0","title":"Algoritma KNN"},{"location":"Statistik Dekriptif/","text":"STATISTIK DESKRIPTIF \u00b6 1. PENGERTIAN Statistik deskritif memberikan informasi tentang data yang dipunyai dan sama sekali tidak menarik inferensia atau kesimpulan apapun tentang gugus induknya yang lebih besar. Dengan statistik ini, kumpulan data yang diperoleh akan tersaji denga ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada. 2. UKURAN TENDENSI SENTRAL Ukuran tendensi sentral adalah setiap pengukuran aritmatika yang ditujukan untuk menggambarkan suatu nilai yang mewakili nilai pusat atau nilai sentral dari suatu gugus data (himpunan pengamatan) berikut adalah bebrapa jenis ukuran tendensi sentral yang sering digunakan: a. Mean Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Dimana: x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyaknya data b. Modus Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana: Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak (selalu positif) c. Median Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut . Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Median dari kelompok data n = banyak data d. Varians Varians adalah suatu ukuran penyebaran data, yang diukur dalam pangkat dua dari selisih data terhadap rata-ratanya. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); e. Standard Deviasi Standar deviasi merupakan akar dari varians (ingat, karena pada varians kita mengkuadratkan selisih data dari rata-ratanya, maka dengan mengakarkannya, kita mendapatkan kembali nilai asalnya). 3. ALAT DAN BAHAN pada tugas kali ini saya menggunakan data random sebanyak 500 data yang disimpan dalam bentuk .csv. Saya juga menggunakan IDLE Python dan Library pandas, matplotlib juga scipy. pandas berguna untuk membaca file .csv dan untuk mengolah data. Mtpolotlib berguna untuk memvisualisasikan data dengan lebih indah dan rapi. Sedangkan scipy berguna untuk menangani operasi aljabar dan matriks serta operasi matematika lainnya. a. Langkah Pertama Memasukkan library from scipy import stats import pandas as pd import matplotlib.pyplot as plt b. Langkah Kedua mengimport data .csv yang telah disipakan df = pd . read_csv ( 'tugas1.csv' , sep = ';' ) df c. Langkah Ketiga Menampung nilai yang akan ditampilkan didalam data penyimpanan (dictionary). Membuat iterasi yang akan mengambil data pada file yangtelah diimport. data = { \"stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard deviasi' , 'Variasi' , 'Skewnes' , 'Q1' , 'Q2' , 'Q3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'stats' ] + [ x for x in df . columns ]) tes d. Langkah Keempat memvisualisasikan hasil tersebut dalam bentuk dataframe data = { \"stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard deviasi' , 'Variasi' , 'Skewnes' , 'Q1' , 'Q2' , 'Q3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'stats' ] + [ x for x in df . columns ]) tes stats x1 x2 x3 x4 0 Min 60.000 60.00 60.000 60.000 1 Max 100.000 100.00 100.000 100.000 2 Mean 79.478 80.29 80.334 80.256 3 Standard deviasi 12.080 11.25 11.540 11.790 4 Variasi 145.990 126.66 133.260 139.080 5 Skewnes 0.060 -0.06 0.010 -0.000 6 Q1 69.000 71.00 71.000 70.000 7 Q2 78.500 81.00 80.500 80.000 8 Q3 90.000 89.00 90.000 90.000 9 Median 78.500 81.00 80.500 80.000 10 Modus 72.000 86.00 71.000 93.000","title":"statistik deskriptif"},{"location":"Statistik Dekriptif/#statistik-deskriptif","text":"1. PENGERTIAN Statistik deskritif memberikan informasi tentang data yang dipunyai dan sama sekali tidak menarik inferensia atau kesimpulan apapun tentang gugus induknya yang lebih besar. Dengan statistik ini, kumpulan data yang diperoleh akan tersaji denga ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada. 2. UKURAN TENDENSI SENTRAL Ukuran tendensi sentral adalah setiap pengukuran aritmatika yang ditujukan untuk menggambarkan suatu nilai yang mewakili nilai pusat atau nilai sentral dari suatu gugus data (himpunan pengamatan) berikut adalah bebrapa jenis ukuran tendensi sentral yang sering digunakan: a. Mean Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Dimana: x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyaknya data b. Modus Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana: Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak (selalu positif) c. Median Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut . Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Median dari kelompok data n = banyak data d. Varians Varians adalah suatu ukuran penyebaran data, yang diukur dalam pangkat dua dari selisih data terhadap rata-ratanya. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); e. Standard Deviasi Standar deviasi merupakan akar dari varians (ingat, karena pada varians kita mengkuadratkan selisih data dari rata-ratanya, maka dengan mengakarkannya, kita mendapatkan kembali nilai asalnya). 3. ALAT DAN BAHAN pada tugas kali ini saya menggunakan data random sebanyak 500 data yang disimpan dalam bentuk .csv. Saya juga menggunakan IDLE Python dan Library pandas, matplotlib juga scipy. pandas berguna untuk membaca file .csv dan untuk mengolah data. Mtpolotlib berguna untuk memvisualisasikan data dengan lebih indah dan rapi. Sedangkan scipy berguna untuk menangani operasi aljabar dan matriks serta operasi matematika lainnya. a. Langkah Pertama Memasukkan library from scipy import stats import pandas as pd import matplotlib.pyplot as plt b. Langkah Kedua mengimport data .csv yang telah disipakan df = pd . read_csv ( 'tugas1.csv' , sep = ';' ) df c. Langkah Ketiga Menampung nilai yang akan ditampilkan didalam data penyimpanan (dictionary). Membuat iterasi yang akan mengambil data pada file yangtelah diimport. data = { \"stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard deviasi' , 'Variasi' , 'Skewnes' , 'Q1' , 'Q2' , 'Q3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'stats' ] + [ x for x in df . columns ]) tes d. Langkah Keempat memvisualisasikan hasil tersebut dalam bentuk dataframe data = { \"stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard deviasi' , 'Variasi' , 'Skewnes' , 'Q1' , 'Q2' , 'Q3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'stats' ] + [ x for x in df . columns ]) tes stats x1 x2 x3 x4 0 Min 60.000 60.00 60.000 60.000 1 Max 100.000 100.00 100.000 100.000 2 Mean 79.478 80.29 80.334 80.256 3 Standard deviasi 12.080 11.25 11.540 11.790 4 Variasi 145.990 126.66 133.260 139.080 5 Skewnes 0.060 -0.06 0.010 -0.000 6 Q1 69.000 71.00 71.000 70.000 7 Q2 78.500 81.00 80.500 80.000 8 Q3 90.000 89.00 90.000 90.000 9 Median 78.500 81.00 80.500 80.000 10 Modus 72.000 86.00 71.000 93.000","title":"STATISTIK DESKRIPTIF"},{"location":"jarak data/","text":"Mengukur Jarak Data \u00b6 Komponen utama dalam algoritma clustering berbasis jarak adalah mengukur jarak antar data. Untuk Melakukan suat pengelompokan algoritma yang dipakai adalah algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering tergantung pada jaraknya. Ukuran kesamaan adalah ukuran seberapa mirip dua objek data. Ukuran kesamaan dalam konteks data mining adalah jarak dengan dimensi yang mewakili fitur dari objek. Jika jarak ini kecil, itu akan menjadi tingkat kesamaan yang tinggi di mana jarak yang besar akan menjadi tingkat kesamaan yang rendah. Kesamaannya adalah subyektif dan sangat tergantung pada domain dan aplikasi. Misalnya, dua buah serupa karena warna atau ukuran atau rasa. Perhatian harus diambil saat menghitung jarak lintas dimensi / fitur yang tidak terkait. Nilai-nilai relatif dari setiap elemen harus dinormalisasi, atau satu fitur bisa berakhir mendominasi perhitungan jarak. Manhattan Distance \u00b6 Mahattan Distance sensitif terhadap outlier. Rumus Mahattan cenderung lebih cepat dibandingkan dengan rumus Euclidean namun dari beberapa percobaan hasil proses clustering Euclidean lebih baik dari Manhattan. $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ Contoh Implementasi Program Python Menggunakan Manhattan Distance \u00b6 from math import * def manhattan_distance ( x , y ): return sum ( abs ( a - b ) for a , b in zip ( x , y )) print manhattan_distance ([ 10 , 20 , 10 ],[ 10 , 20 , 20 ]) 10 [ Finished in 0.0 s ] Euclidean Distance \u00b6 metode ini merupakan metode paling terkenal. untuk mempelajari hubungan antara sudut dan jarak. Euclidean distance adalah metode yang lebih baik dari pada Mahattan Distance. Jarak Euclidean adalah penggunaan jarak yang paling umum. Dalam kebanyakan kasus ketika orang mengatakan tentang jarak, mereka akan merujuk pada jarak Euclidean. Jarak Euclidean juga dikenal sebagai jarak sederhana. Ketika data padat atau kontinu, ini adalah ukuran kedekatan terbaik. Jarak Euclidean antara dua titik adalah panjang jalur yang menghubungkannya. Teorema Pythagoras memberikan jarak ini antara dua titik. $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); Contoh Implementasi Euclidean Distance pada python \u00b6 from math import * def euclidean_distance ( x , y ): return sqrt ( sum ( pow ( a - b , 2 ) for a , b in zip ( x , y ))) print euclidean_distance ([ 0 , 3 , 4 , 5 ],[ 7 , 6 , 3 , - 1 ]) 9.74679434481 [ Finished in 0.0 s ] Manhattan distance implementation in python: \u00b6 def manhattan_distance ( x , y ): return sum ( abs ( a - b ) for a , b in zip ( x , y )) print manhattan_distance ([ 10 , 20 , 10 ],[ 10 , 20 , 20 ]) 10 [ Finished in 0.0 s ] Minkowski Distance \u00b6 Jarak Minkowski adalah bentuk metrik umum jarak Euclidean dan jarak Manhattan. Cara jarak diukur dengan metrik Minkowski dengan urutan berbeda antara dua objek dengan tiga variabel (Pada gambar ditampilkan dalam sistem koordinat dengan sumbu x, y, z). $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ Minkowski distance implementation in python: \u00b6 from math import * from decimal import Decimal def nth_root ( value , n_root ): root_value = 1 / float ( n_root ) return round ( Decimal ( value ) ** Decimal ( root_value ), 3 ) def minkowski_distance ( x , y , p_value ): return nth_root ( sum ( pow ( abs ( a - b ), p_value ) for a , b in zip ( x , y )), p_value ) print minkowski_distance ([ 0 , 3 , 4 , 5 ],[ 7 , 6 , 3 , - 1 ], 3 )","title":"Mengukur Jarak Data"},{"location":"jarak data/#mengukur-jarak-data","text":"Komponen utama dalam algoritma clustering berbasis jarak adalah mengukur jarak antar data. Untuk Melakukan suat pengelompokan algoritma yang dipakai adalah algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering tergantung pada jaraknya. Ukuran kesamaan adalah ukuran seberapa mirip dua objek data. Ukuran kesamaan dalam konteks data mining adalah jarak dengan dimensi yang mewakili fitur dari objek. Jika jarak ini kecil, itu akan menjadi tingkat kesamaan yang tinggi di mana jarak yang besar akan menjadi tingkat kesamaan yang rendah. Kesamaannya adalah subyektif dan sangat tergantung pada domain dan aplikasi. Misalnya, dua buah serupa karena warna atau ukuran atau rasa. Perhatian harus diambil saat menghitung jarak lintas dimensi / fitur yang tidak terkait. Nilai-nilai relatif dari setiap elemen harus dinormalisasi, atau satu fitur bisa berakhir mendominasi perhitungan jarak.","title":"Mengukur Jarak Data"},{"location":"jarak data/#manhattan-distance","text":"Mahattan Distance sensitif terhadap outlier. Rumus Mahattan cenderung lebih cepat dibandingkan dengan rumus Euclidean namun dari beberapa percobaan hasil proses clustering Euclidean lebih baik dari Manhattan. $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$","title":"Manhattan Distance"},{"location":"jarak data/#contoh-implementasi-program-python-menggunakan-manhattan-distance","text":"from math import * def manhattan_distance ( x , y ): return sum ( abs ( a - b ) for a , b in zip ( x , y )) print manhattan_distance ([ 10 , 20 , 10 ],[ 10 , 20 , 20 ]) 10 [ Finished in 0.0 s ]","title":"Contoh Implementasi Program Python Menggunakan Manhattan Distance"},{"location":"jarak data/#euclidean-distance","text":"metode ini merupakan metode paling terkenal. untuk mempelajari hubungan antara sudut dan jarak. Euclidean distance adalah metode yang lebih baik dari pada Mahattan Distance. Jarak Euclidean adalah penggunaan jarak yang paling umum. Dalam kebanyakan kasus ketika orang mengatakan tentang jarak, mereka akan merujuk pada jarak Euclidean. Jarak Euclidean juga dikenal sebagai jarak sederhana. Ketika data padat atau kontinu, ini adalah ukuran kedekatan terbaik. Jarak Euclidean antara dua titik adalah panjang jalur yang menghubungkannya. Teorema Pythagoras memberikan jarak ini antara dua titik. $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Euclidean Distance"},{"location":"jarak data/#contoh-implementasi-euclidean-distance-pada-python","text":"from math import * def euclidean_distance ( x , y ): return sqrt ( sum ( pow ( a - b , 2 ) for a , b in zip ( x , y ))) print euclidean_distance ([ 0 , 3 , 4 , 5 ],[ 7 , 6 , 3 , - 1 ]) 9.74679434481 [ Finished in 0.0 s ]","title":"Contoh Implementasi Euclidean Distance pada python"},{"location":"jarak data/#manhattan-distance-implementation-in-python","text":"def manhattan_distance ( x , y ): return sum ( abs ( a - b ) for a , b in zip ( x , y )) print manhattan_distance ([ 10 , 20 , 10 ],[ 10 , 20 , 20 ]) 10 [ Finished in 0.0 s ]","title":"Manhattan distance implementation in python:"},{"location":"jarak data/#minkowski-distance","text":"Jarak Minkowski adalah bentuk metrik umum jarak Euclidean dan jarak Manhattan. Cara jarak diukur dengan metrik Minkowski dengan urutan berbeda antara dua objek dengan tiga variabel (Pada gambar ditampilkan dalam sistem koordinat dengan sumbu x, y, z). $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$","title":"Minkowski Distance"},{"location":"jarak data/#minkowski-distance-implementation-in-python","text":"from math import * from decimal import Decimal def nth_root ( value , n_root ): root_value = 1 / float ( n_root ) return round ( Decimal ( value ) ** Decimal ( root_value ), 3 ) def minkowski_distance ( x , y , p_value ): return nth_root ( sum ( pow ( abs ( a - b ), p_value ) for a , b in zip ( x , y )), p_value ) print minkowski_distance ([ 0 , 3 , 4 , 5 ],[ 7 , 6 , 3 , - 1 ], 3 )","title":"Minkowski distance implementation in python:"}]}
{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nama : Nadia Asri NIM : 180411100063 Kelas : Penambangan Data D","title":"index"},{"location":"Decision Tree/","text":"DECISION TREES \u00b6 Classification \u00b6 Decision tree membangun model klasifikasi atau regresi dalam bentuk struktur pohon. Ini memecah dataset menjadi himpunan bagian yang lebih kecil dan lebih kecil sementara pada saat yang sama pohon keputusan terkait dikembangkan secara bertahap. Hasil akhirnya adalah pohon dengan simpul keputusan dan simpul daun. Node keputusan (mis., Outlook) memiliki dua atau lebih cabang (mis., Sunny, Overcast, dan Rainy). Node daun (mis., Play) mewakili klasifikasi atau keputusan. Node keputusan teratas dalam pohon yang sesuai dengan prediktor terbaik disebut simpul akar. Pohon keputusan dapat menangani data kategorikal dan numerik. Decision Tree terdiri dari: \u00b6 Node: Tes untuk nilai atribut tertentu. Edges / Branch Sesuai dengan hasil tes dan terhubung ke simpul atau daun berikutnya. Leaf Node: Node terminal yang memprediksi hasil (mewakili label kelas atau distribusi kelas). Algorithm \u00b6 Algoritma inti untuk membangun pohon keputusan yang disebut ID3 oleh J. R. Quinlan yang menggunakan pencarian serakah top-down melalui ruang cabang yang mungkin tanpa backtracking. ID3 menggunakan Entropi dan Penguatan Informasi untuk membangun pohon keputusan. Dalam model ZeroR tidak ada prediktor, dalam model OneR kami mencoba menemukan prediktor tunggal terbaik, Bayesian naif mencakup semua prediktor yang menggunakan aturan Bayes dan asumsi independensi antara prediktor, tetapi pohon keputusan mencakup semua prediktor dengan asumsi ketergantungan antara prediktor. Entropy \u00b6 Pohon keputusan dibangun dari atas ke bawah dari simpul akar dan melibatkan mempartisi data ke dalam himpunan bagian yang berisi instance dengan nilai yang sama (homogen). Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi rata, entropinya satu. $$ Entropy(S) = \\sum_{i=1}^n {-P_i\\log_2{P_i}} $$ Keterangan : S = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Pi = Probability dari Pi terhadap P Information Gain \u00b6 Gain informasi didasarkan pada penurunan entropi setelah dataset dibagi pada atribut. Membangun pohon keputusan adalah tentang menemukan atribut yang mengembalikan perolehan informasi tertinggi (mis., Cabang yang paling homogen). $$ Gain(S,A) = entropy(S)-\\sum_{i=1}^n \\frac{|s_i|}{|s|}\\quad x \\quad entropy(S_i) $$ Keterangan : T = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Si = Probability dari Si terhadap S Data Berikut untuk menghitung Entropy,Gain yang ada di tiap data tersebut menggunakan hitugan manual : \u00b6 Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Kita harus menghitung Entropy terlebih dahulu dengan menggunakan rumus Entropy(S) $$ P_i = C0 = , P(C0)=P_1 = \\frac{10}{20} $$ $$ P_i = C0 = , P(C1)=P_2 = \\frac{10}{20} $$ $$ Entropy(S) = {-\\frac{10}{20}\\log_2{\\frac{10}{20}}}{-\\frac{10}{20}\\log_2{\\frac{10}{20}}}= 1 $$ Sudah dicari Entropynya = 1 maka selanjutnya mencari tiap Gainnya Gain dari Gender dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari CarType dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari Shirt Size dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value $$ Entropy(Male) = {-\\frac{6}{10}\\log_2{\\frac{6}{10}}}{-\\frac{4}{10}\\log_2{\\frac{4}{10}}}=0,97051 $$ $$ Entropy(Female) = {-\\frac{4}{10}\\log_2{\\frac{4}{10}}}{-\\frac{6}{10}\\log_2{\\frac{6}{10}}}=0,97051 $$ $$ Entropy(Family) = {-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}}=0,81128 $$ $$ Entropy(Sports) = {-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}=0 $$ $$ Entropy(Luxury) = {-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}=0,54356 $$ $$ Gain(S,CarType) = 1-(\\frac{4}{20}X({-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}})+\\frac{8}{20}X({-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}+\\frac{8}{20}X({-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}) = 0,62032 $$ $$ Entropy(Small) = {-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}}=0,97095 $$ $$ Entropy(Medium) = {-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}=0,98523 $$ $$ Entropy(Large) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Entropy(ExtraLarge) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Gain(S,ShirtSize) = 1-(\\frac{5}{20}X({-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}})+\\frac{7}{20}X({-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}) = 0,012433 $$ Program untuk menghitung entropy, gain dengan program: import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.tree import DecisionTreeClassifier from sklearn import tree data = pd . read_excel ( \"data_informationgain.xlsx\" ) df = pd . DataFrame ( data ) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Merubah label / kolom yang ada di data diatas untuk bisa dihitung dengan menggunakan code python,yaitu untuk memudahkan pengguna agar bisa menghitung code yang nanti akan digunakan . Pada gender hanya menggunakan biner (0,1){F,M}. Pada cartype menggunakan inisialisasi angka urutan (0,1,2){Familiy,Luxury,Sports} . Pada Shirt Size menggunakan inisialisasi angka urutan (0,1,2,3){Small,Medium,Large,ExtraLarge}.Berikut codenya lab = LabelEncoder () df [ \"gender_n\" ] = lab . fit_transform ( df [ \"Gender\" ]) df [ \"car_type_n\" ] = lab . fit_transform ( df [ \"Car Type\" ]) df [ \"shirt_size_n\" ] = lab . fit_transform ( df [ \"Shirt Size\" ]) df [ \"class_n\" ] = lab . fit_transform ( df [ \"Class\" ]) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class gender_n car_type_n shirt_size_n class_n 1 M Family Small C0 1 0 3 0 2 M Sports Medium C0 1 2 2 0 3 M Sports Medium C0 1 2 2 0 4 M Sports Large C0 1 2 1 0 5 M Sports Extra Large C0 1 2 0 0 6 M Sports Extra Large C0 1 2 0 0 7 F Sports Small C0 0 2 3 0 8 F Sports Small C0 0 2 3 0 9 F Sports Medium C0 0 2 2 0 10 F Luxury Large C0 0 1 1 0 11 M Family Large C1 1 0 1 1 12 M Family Extra Large C1 1 0 0 1 13 M Family Medium C1 1 0 2 1 14 M Luxury Extra Large C1 1 1 0 1 15 F Luxury Small C1 0 1 3 1 16 F Luxury Small C1 0 1 3 1 17 F Luxury Medium C1 0 1 2 1 18 F Luxury Medium C1 0 1 2 1 19 F Luxury Medium C1 0 1 2 1 20 F Luxury Large C1 0 1 Setelah merubah label menjadi inisialisasi angka (numerik) melanjutkan ke code berikutnya untuk bisa dihitung dan code berikut untuk penghapusan fitur /label yang tidak diperlukan inputs = df . drop ([ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" , \"Class\" , \"class_n\" ], axis = \"columns\" ) target = df [ \"class_n\" ] Membuat Klasifikasi fitur untuk bisa dibuat Pohon Keputusan (Decision Tree)untuk bisa di classifier . model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best') from matplotlib import pyplot as plt tree . plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" ], class_names = [ \"C0\" , \"C1\" ], label = \"all\" , filled = True , impurity = True , node_ids = True , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"statistik deskriptif"},{"location":"Decision Tree/#decision-trees","text":"","title":"DECISION TREES"},{"location":"Decision Tree/#classification","text":"Decision tree membangun model klasifikasi atau regresi dalam bentuk struktur pohon. Ini memecah dataset menjadi himpunan bagian yang lebih kecil dan lebih kecil sementara pada saat yang sama pohon keputusan terkait dikembangkan secara bertahap. Hasil akhirnya adalah pohon dengan simpul keputusan dan simpul daun. Node keputusan (mis., Outlook) memiliki dua atau lebih cabang (mis., Sunny, Overcast, dan Rainy). Node daun (mis., Play) mewakili klasifikasi atau keputusan. Node keputusan teratas dalam pohon yang sesuai dengan prediktor terbaik disebut simpul akar. Pohon keputusan dapat menangani data kategorikal dan numerik.","title":"Classification"},{"location":"Decision Tree/#decision-tree-terdiri-dari","text":"Node: Tes untuk nilai atribut tertentu. Edges / Branch Sesuai dengan hasil tes dan terhubung ke simpul atau daun berikutnya. Leaf Node: Node terminal yang memprediksi hasil (mewakili label kelas atau distribusi kelas).","title":"Decision Tree terdiri dari:"},{"location":"Decision Tree/#algorithm","text":"Algoritma inti untuk membangun pohon keputusan yang disebut ID3 oleh J. R. Quinlan yang menggunakan pencarian serakah top-down melalui ruang cabang yang mungkin tanpa backtracking. ID3 menggunakan Entropi dan Penguatan Informasi untuk membangun pohon keputusan. Dalam model ZeroR tidak ada prediktor, dalam model OneR kami mencoba menemukan prediktor tunggal terbaik, Bayesian naif mencakup semua prediktor yang menggunakan aturan Bayes dan asumsi independensi antara prediktor, tetapi pohon keputusan mencakup semua prediktor dengan asumsi ketergantungan antara prediktor.","title":"Algorithm"},{"location":"Decision Tree/#entropy","text":"Pohon keputusan dibangun dari atas ke bawah dari simpul akar dan melibatkan mempartisi data ke dalam himpunan bagian yang berisi instance dengan nilai yang sama (homogen). Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi rata, entropinya satu. $$ Entropy(S) = \\sum_{i=1}^n {-P_i\\log_2{P_i}} $$ Keterangan : S = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Pi = Probability dari Pi terhadap P","title":"Entropy"},{"location":"Decision Tree/#information-gain","text":"Gain informasi didasarkan pada penurunan entropi setelah dataset dibagi pada atribut. Membangun pohon keputusan adalah tentang menemukan atribut yang mengembalikan perolehan informasi tertinggi (mis., Cabang yang paling homogen). $$ Gain(S,A) = entropy(S)-\\sum_{i=1}^n \\frac{|s_i|}{|s|}\\quad x \\quad entropy(S_i) $$ Keterangan : T = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Si = Probability dari Si terhadap S","title":"Information Gain"},{"location":"Decision Tree/#data-berikut-untuk-menghitung-entropygain-yang-ada-di-tiap-data-tersebut-menggunakan-hitugan-manual","text":"Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Kita harus menghitung Entropy terlebih dahulu dengan menggunakan rumus Entropy(S) $$ P_i = C0 = , P(C0)=P_1 = \\frac{10}{20} $$ $$ P_i = C0 = , P(C1)=P_2 = \\frac{10}{20} $$ $$ Entropy(S) = {-\\frac{10}{20}\\log_2{\\frac{10}{20}}}{-\\frac{10}{20}\\log_2{\\frac{10}{20}}}= 1 $$ Sudah dicari Entropynya = 1 maka selanjutnya mencari tiap Gainnya Gain dari Gender dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari CarType dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari Shirt Size dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value $$ Entropy(Male) = {-\\frac{6}{10}\\log_2{\\frac{6}{10}}}{-\\frac{4}{10}\\log_2{\\frac{4}{10}}}=0,97051 $$ $$ Entropy(Female) = {-\\frac{4}{10}\\log_2{\\frac{4}{10}}}{-\\frac{6}{10}\\log_2{\\frac{6}{10}}}=0,97051 $$ $$ Entropy(Family) = {-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}}=0,81128 $$ $$ Entropy(Sports) = {-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}=0 $$ $$ Entropy(Luxury) = {-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}=0,54356 $$ $$ Gain(S,CarType) = 1-(\\frac{4}{20}X({-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}})+\\frac{8}{20}X({-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}+\\frac{8}{20}X({-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}) = 0,62032 $$ $$ Entropy(Small) = {-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}}=0,97095 $$ $$ Entropy(Medium) = {-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}=0,98523 $$ $$ Entropy(Large) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Entropy(ExtraLarge) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Gain(S,ShirtSize) = 1-(\\frac{5}{20}X({-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}})+\\frac{7}{20}X({-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}) = 0,012433 $$ Program untuk menghitung entropy, gain dengan program: import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.tree import DecisionTreeClassifier from sklearn import tree data = pd . read_excel ( \"data_informationgain.xlsx\" ) df = pd . DataFrame ( data ) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Merubah label / kolom yang ada di data diatas untuk bisa dihitung dengan menggunakan code python,yaitu untuk memudahkan pengguna agar bisa menghitung code yang nanti akan digunakan . Pada gender hanya menggunakan biner (0,1){F,M}. Pada cartype menggunakan inisialisasi angka urutan (0,1,2){Familiy,Luxury,Sports} . Pada Shirt Size menggunakan inisialisasi angka urutan (0,1,2,3){Small,Medium,Large,ExtraLarge}.Berikut codenya lab = LabelEncoder () df [ \"gender_n\" ] = lab . fit_transform ( df [ \"Gender\" ]) df [ \"car_type_n\" ] = lab . fit_transform ( df [ \"Car Type\" ]) df [ \"shirt_size_n\" ] = lab . fit_transform ( df [ \"Shirt Size\" ]) df [ \"class_n\" ] = lab . fit_transform ( df [ \"Class\" ]) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class gender_n car_type_n shirt_size_n class_n 1 M Family Small C0 1 0 3 0 2 M Sports Medium C0 1 2 2 0 3 M Sports Medium C0 1 2 2 0 4 M Sports Large C0 1 2 1 0 5 M Sports Extra Large C0 1 2 0 0 6 M Sports Extra Large C0 1 2 0 0 7 F Sports Small C0 0 2 3 0 8 F Sports Small C0 0 2 3 0 9 F Sports Medium C0 0 2 2 0 10 F Luxury Large C0 0 1 1 0 11 M Family Large C1 1 0 1 1 12 M Family Extra Large C1 1 0 0 1 13 M Family Medium C1 1 0 2 1 14 M Luxury Extra Large C1 1 1 0 1 15 F Luxury Small C1 0 1 3 1 16 F Luxury Small C1 0 1 3 1 17 F Luxury Medium C1 0 1 2 1 18 F Luxury Medium C1 0 1 2 1 19 F Luxury Medium C1 0 1 2 1 20 F Luxury Large C1 0 1 Setelah merubah label menjadi inisialisasi angka (numerik) melanjutkan ke code berikutnya untuk bisa dihitung dan code berikut untuk penghapusan fitur /label yang tidak diperlukan inputs = df . drop ([ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" , \"Class\" , \"class_n\" ], axis = \"columns\" ) target = df [ \"class_n\" ] Membuat Klasifikasi fitur untuk bisa dibuat Pohon Keputusan (Decision Tree)untuk bisa di classifier . model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best') from matplotlib import pyplot as plt tree . plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" ], class_names = [ \"C0\" , \"C1\" ], label = \"all\" , filled = True , impurity = True , node_ids = True , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Data Berikut untuk menghitung Entropy,Gain yang ada di tiap data tersebut menggunakan hitugan manual :"},{"location":"Eror Dalam Komputasi Numerik/","text":"Eror Dalam Komputasi Numerik \u00b6 Pengertian \u00b6 Komputer Numerik bertujuan untuk menentukan suatu akurasi dari hasil perhitungan atau percobaan. Komputer numerik dapat dikatakan sebagai komputasi yang mengikuti suatu algoritma pendekatan ( aproksimasi ) untuk menyelesaikan suatu persoalan, yang dengan demikian besar kemungkinan di situ terkandung \u201ckesalahan\u201d. Error/Galat merupakan perbedaan antara hasil penyelesaian suatu model matematik secara numeric dengan penyelesaian secara analitis. Nilai Galat (Nilai Kesalahan) \u00b6 Besarnya kesalahan atas suatu nilai taksiran dapat dinyatakan secara kuantitatif dan kualitatif. Besarnya kesalahan yang dinyatakan secara kuantitatif disebut Kesalahan Absolut. Besarnya kesalahan yang dinyatakan secara kualitatif disebut dengan Kesalahan Relatif . Absolute Error \u00b6 Kesalahan absolut suatu kuantitas adalah nilai absolut dari selisih antara nilai sebenarnya X dan nilai perkiraan x. Ini dilambangkan dengan Relative Error \u00b6 Relative error biasa disebut sebagai kesalahan relatif dari suatu kuantitas adalah rasio kesalahan absolutnya terhadap nilai sebenarnya. Penyabab Terjadinya Error \u00b6 Round-off-errors Truncation errors Inherent error import math x = int ( input ( \"masukan nilai x=\" )) coba = 1 a = 0 b = 1 while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 3 * i ) * x * i / math . factorial ( i ) for j in range ( b ): f_y += ( 3 * j ) * x * j / math . factorial ( j ) print ( \"suku ke\" , a , \"=\" , f_x ) print ( \"suku ke\" , b , \"=\" , f_y ) coba = f_y - f_x a += 1 b += 1 print ( \"hasil seliih =\" , coba )","title":"Eror Dalam Komputasi Numerik"},{"location":"Eror Dalam Komputasi Numerik/#eror-dalam-komputasi-numerik","text":"","title":"Eror Dalam Komputasi Numerik"},{"location":"Eror Dalam Komputasi Numerik/#pengertian","text":"Komputer Numerik bertujuan untuk menentukan suatu akurasi dari hasil perhitungan atau percobaan. Komputer numerik dapat dikatakan sebagai komputasi yang mengikuti suatu algoritma pendekatan ( aproksimasi ) untuk menyelesaikan suatu persoalan, yang dengan demikian besar kemungkinan di situ terkandung \u201ckesalahan\u201d. Error/Galat merupakan perbedaan antara hasil penyelesaian suatu model matematik secara numeric dengan penyelesaian secara analitis.","title":"Pengertian"},{"location":"Eror Dalam Komputasi Numerik/#nilai-galat-nilai-kesalahan","text":"Besarnya kesalahan atas suatu nilai taksiran dapat dinyatakan secara kuantitatif dan kualitatif. Besarnya kesalahan yang dinyatakan secara kuantitatif disebut Kesalahan Absolut. Besarnya kesalahan yang dinyatakan secara kualitatif disebut dengan Kesalahan Relatif .","title":"Nilai Galat (Nilai Kesalahan)"},{"location":"Eror Dalam Komputasi Numerik/#absolute-error","text":"Kesalahan absolut suatu kuantitas adalah nilai absolut dari selisih antara nilai sebenarnya X dan nilai perkiraan x. Ini dilambangkan dengan","title":"Absolute Error"},{"location":"Eror Dalam Komputasi Numerik/#relative-error","text":"Relative error biasa disebut sebagai kesalahan relatif dari suatu kuantitas adalah rasio kesalahan absolutnya terhadap nilai sebenarnya.","title":"Relative Error"},{"location":"Eror Dalam Komputasi Numerik/#penyabab-terjadinya-error","text":"Round-off-errors Truncation errors Inherent error import math x = int ( input ( \"masukan nilai x=\" )) coba = 1 a = 0 b = 1 while coba > 0.001 : f_x = 0 f_y = 0 for i in range ( a ): f_x += ( 3 * i ) * x * i / math . factorial ( i ) for j in range ( b ): f_y += ( 3 * j ) * x * j / math . factorial ( j ) print ( \"suku ke\" , a , \"=\" , f_x ) print ( \"suku ke\" , b , \"=\" , f_y ) coba = f_y - f_x a += 1 b += 1 print ( \"hasil seliih =\" , coba )","title":"Penyabab Terjadinya Error"},{"location":"K-Means Clustering/","text":"K-Means Clustering \u00b6 Pendahuluan \u00b6 K-Means salah satu metode penganalisaan data yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. untuk mencapai ini, kita akan menggunakan algoritma K-Means; algoritma pembelajaran tanpa pengawasan. Algoritma \u00b6 Algoritma K-Means Clustering akan mengkategorikan item ke dalam kelompok k kesamaan. Untuk menghitung kesamaan itu, akan menggunakan jarak euclidean sebagai pengukuran. Algoritma bekerja sebagai berikut: menentukan jumlah cluster menginisialisasi poin k secara acak ke dalam cluster menghitung rata-rata yang dari setiap cluster mengelompokkan masing-masing data ke dalam rata-rata terdekat mengulangi langkah ketiga sampai tidak ada data yeng berpindah ketika rata-rata selesai dihitung. Langkah-langkah untuk melakukan K-Means Clustering \u00b6 Langkah-langkah akan dilakukan tanpa mengimport library apapun. jadi pada tugas kali ini, akan melakukan coding murni menggunakan bahasa pemograman Python. Membaca data program menerima input dari file sebagai text ('.txt'). Setiap baris mewakili suatu item, dan itu berisi nilai numerik (satu untuk setiap fitur) dibagi dengan koma. html 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 5.4,3.7,1.5,0.2,Iris-setosa 4.8,3.4,1.6,0.2,Iris-setosa 4.8,3.0,1.4,0.1,Iris-setosa 4.3,3.0,1.1,0.1,Iris-setosa 5.8,4.0,1.2,0.2,Iris-setosa 5.7,4.4,1.5,0.4,Iris-setosa 5.4,3.9,1.3,0.4,Iris-setosa 5.1,3.5,1.4,0.3,Iris-setosa 5.7,3.8,1.7,0.3,Iris-setosa 5.1,3.8,1.5,0.3,Iris-setosa 5.4,3.4,1.7,0.2,Iris-setosa 5.1,3.7,1.5,0.4,Iris-setosa 4.6,3.6,1.0,0.2,Iris-setosa 5.1,3.3,1.7,0.5,Iris-setosa 4.8,3.4,1.9,0.2,Iris-setosa 5.0,3.0,1.6,0.2,Iris-setosa 5.0,3.4,1.6,0.4,Iris-setosa 5.2,3.5,1.5,0.2,Iris-setosa 5.2,3.4,1.4,0.2,Iris-setosa 4.7,3.2,1.6,0.2,Iris-setosa 4.8,3.1,1.6,0.2,Iris-setosa 5.4,3.4,1.5,0.4,Iris-setosa 5.2,4.1,1.5,0.1,Iris-setosa 5.5,4.2,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 5.0,3.2,1.2,0.2,Iris-setosa 5.5,3.5,1.3,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 4.4,3.0,1.3,0.2,Iris-setosa 5.1,3.4,1.5,0.2,Iris-setosa 5.0,3.5,1.3,0.3,Iris-setosa 4.5,2.3,1.3,0.3,Iris-setosa 4.4,3.2,1.3,0.2,Iris-setosa 5.0,3.5,1.6,0.6,Iris-setosa 5.1,3.8,1.9,0.4,Iris-setosa 4.8,3.0,1.4,0.3,Iris-setosa 5.1,3.8,1.6,0.2,Iris-setosa 4.6,3.2,1.4,0.2,Iris-setosa 5.3,3.7,1.5,0.2,Iris-setosa 5.0,3.3,1.4,0.2,Iris-setosa 7.0,3.2,4.7,1.4,Iris-versicolor 6.4,3.2,4.5,1.5,Iris-versicolor 6.9,3.1,4.9,1.5,Iris-versicolor 5.5,2.3,4.0,1.3,Iris-versicolor 6.5,2.8,4.6,1.5,Iris-versicolor 5.7,2.8,4.5,1.3,Iris-versicolor 6.3,3.3,4.7,1.6,Iris-versicolor 4.9,2.4,3.3,1.0,Iris-versicolor 6.6,2.9,4.6,1.3,Iris-versicolor 5.2,2.7,3.9,1.4,Iris-versicolor 5.0,2.0,3.5,1.0,Iris-versicolor 5.9,3.0,4.2,1.5,Iris-versicolor 6.0,2.2,4.0,1.0,Iris-versicolor 6.1,2.9,4.7,1.4,Iris-versicolor 5.6,2.9,3.6,1.3,Iris-versicolor 6.7,3.1,4.4,1.4,Iris-versicolor 5.6,3.0,4.5,1.5,Iris-versicolor 5.8,2.7,4.1,1.0,Iris-versicolor 6.2,2.2,4.5,1.5,Iris-versicolor 5.6,2.5,3.9,1.1,Iris-versicolor 5.9,3.2,4.8,1.8,Iris-versicolor 6.1,2.8,4.0,1.3,Iris-versicolor 6.3,2.5,4.9,1.5,Iris-versicolor 6.1,2.8,4.7,1.2,Iris-versicolor 6.4,2.9,4.3,1.3,Iris-versicolor 6.6,3.0,4.4,1.4,Iris-versicolor 6.8,2.8,4.8,1.4,Iris-versicolor 6.7,3.0,5.0,1.7,Iris-versicolor 6.0,2.9,4.5,1.5,Iris-versicolor 5.7,2.6,3.5,1.0,Iris-versicolor 5.5,2.4,3.8,1.1,Iris-versicolor 5.5,2.4,3.7,1.0,Iris-versicolor 5.8,2.7,3.9,1.2,Iris-versicolor 6.0,2.7,5.1,1.6,Iris-versicolor 5.4,3.0,4.5,1.5,Iris-versicolor 6.0,3.4,4.5,1.6,Iris-versicolor 6.7,3.1,4.7,1.5,Iris-versicolor 6.3,2.3,4.4,1.3,Iris-versicolor 5.6,3.0,4.1,1.3,Iris-versicolor 5.5,2.5,4.0,1.3,Iris-versicolor 5.5,2.6,4.4,1.2,Iris-versicolor 6.1,3.0,4.6,1.4,Iris-versicolor 5.8,2.6,4.0,1.2,Iris-versicolor 5.0,2.3,3.3,1.0,Iris-versicolor 5.6,2.7,4.2,1.3,Iris-versicolor 5.7,3.0,4.2,1.2,Iris-versicolor 5.7,2.9,4.2,1.3,Iris-versicolor 6.2,2.9,4.3,1.3,Iris-versicolor 5.1,2.5,3.0,1.1,Iris-versicolor 5.7,2.8,4.1,1.3,Iris-versicolor 6.3,3.3,6.0,2.5,Iris-virginica 5.8,2.7,5.1,1.9,Iris-virginica 7.1,3.0,5.9,2.1,Iris-virginica 6.3,2.9,5.6,1.8,Iris-virginica 6.5,3.0,5.8,2.2,Iris-virginica 7.6,3.0,6.6,2.1,Iris-virginica 4.9,2.5,4.5,1.7,Iris-virginica 7.3,2.9,6.3,1.8,Iris-virginica 6.7,2.5,5.8,1.8,Iris-virginica 7.2,3.6,6.1,2.5,Iris-virginica 6.5,3.2,5.1,2.0,Iris-virginica 6.4,2.7,5.3,1.9,Iris-virginica 6.8,3.0,5.5,2.1,Iris-virginica 5.7,2.5,5.0,2.0,Iris-virginica 5.8,2.8,5.1,2.4,Iris-virginica 6.4,3.2,5.3,2.3,Iris-virginica 6.5,3.0,5.5,1.8,Iris-virginica 7.7,3.8,6.7,2.2,Iris-virginica 7.7,2.6,6.9,2.3,Iris-virginica 6.0,2.2,5.0,1.5,Iris-virginica 6.9,3.2,5.7,2.3,Iris-virginica 5.6,2.8,4.9,2.0,Iris-virginica 7.7,2.8,6.7,2.0,Iris-virginica 6.3,2.7,4.9,1.8,Iris-virginica 6.7,3.3,5.7,2.1,Iris-virginica 7.2,3.2,6.0,1.8,Iris-virginica 6.2,2.8,4.8,1.8,Iris-virginica 6.1,3.0,4.9,1.8,Iris-virginica 6.4,2.8,5.6,2.1,Iris-virginica 7.2,3.0,5.8,1.6,Iris-virginica 7.4,2.8,6.1,1.9,Iris-virginica 7.9,3.8,6.4,2.0,Iris-virginica 6.4,2.8,5.6,2.2,Iris-virginica 6.3,2.8,5.1,1.5,Iris-virginica 6.1,2.6,5.6,1.4,Iris-virginica 7.7,3.0,6.1,2.3,Iris-virginica 6.3,3.4,5.6,2.4,Iris-virginica 6.4,3.1,5.5,1.8,Iris-virginica 6.0,3.0,4.8,1.8,Iris-virginica 6.9,3.1,5.4,2.1,Iris-virginica 6.7,3.1,5.6,2.4,Iris-virginica 6.9,3.1,5.1,2.3,Iris-virginica 5.8,2.7,5.1,1.9,Iris-virginica 6.8,3.2,5.9,2.3,Iris-virginica 6.7,3.3,5.7,2.5,Iris-virginica 6.7,3.0,5.2,2.3,Iris-virginica 6.3,2.5,5.0,1.9,Iris-virginica 6.5,3.0,5.2,2.0,Iris-virginica 6.2,3.4,5.4,2.3,Iris-virginica 5.9,3.0,5.1,1.8,Iris-virginica Program akan membaca data dari file, menyimpannya ke dalam daftar. Setiap elemen daftar adalah daftar lain yang berisi nilai item untuk fitur. Program melakukan ini dengan fungsi berikut: python def ReadData(fileName): # Read the file, splitting by lines f = open (fileName, 'r' ); lines = f.read().splitlines(); f.close(); items = []; for i in range ( 1 , len (lines)): line = lines[i].split( ',' ); itemFeatures = []; for j in range ( len (line) - 1 ): v = float (line[j]); # Convert feature value to float itemFeatures.append(v); # Add feature value to dict items.append(itemFeatures); shuffle(items); return items; Inisialisasi poin K yang dipilih secara acak Program akan menginisialisasi masing-masing nilai rata-rata dalam kisaran nilai fitur item. Untuk itu, program perlu menemukan min dan maks untuk setiap fitur. Program mencapainya dengan fungsi berikut: python def FindColMinMax(items): n = len (items[ 0 ]); minima = [sys.maxint for i in range (n)]; maxima = [ - sys.maxint - 1 for i in range (n)]; for item in items: for f in range ( len (item)): if (item[f] < minima[f]): minima[f] = item[f]; if (item[f] > maxima[f]): maxima[f] = item[f]; return minima,maxima; Variabel minima, maxima adalah daftar yang berisi nilai min dan max masing-masing item. Program menginisialisasi setiap nilai fitur rata-rata secara acak antara minimum dan maksimum yang sesuai pada dua daftar di atas: python def InitializeMeans(items, k, cMin, cMax): # Initialize means to random numbers between # the min and max of each column/feature f = len (items[ 0 ]); # number of features means = [[ 0 for i in range (f)] for j in range (k)]; for mean in means: for i in range ( len (mean)): # Set value to a random float # (adding +-1 to avoid a wide placement of a mean) mean[i] = uniform(cMin[i] + 1 , cMax[i] - 1 ); return means; Menghitung jarak Euclidean Program akan menggunakan jarak euclidean sebagai metrik kesamaan untuk set data kami (catatan: tergantung pada item Anda, Anda dapat menggunakan metrik kesamaan lainnya). ```python def EuclideanDistance(x, y): S = 0 ; # The sum of the squared differences of the elements for i in range ( len (x)): S + = math. pow (x[i] - y[i], 2 ); return math.sqrt(S); #The square root of the sum ``` Menghitung rata-rata ```python def UpdateMean(n,mean,item): for i in range ( len (mean)): m = mean[i]; m = (m * (n - 1 ) + item[i]) / float (n); mean[i] = round (m, 3 ); return mean; ``` Klasifikasi Item ```python def Classify(means,item): # Classify item to the mean with minimum distance minimum = sys.maxint; index = - 1 ; for i in range ( len (means)): # Find distance from item to mean dis = EuclideanDistance(item, means[i]); if (dis < minimum): minimum = dis; index = i; index; return index; ``` Menghitung kembali rata-rata cluster Untuk benar-benar menemukan rata-rata, program akan mengulang semua item, mengklasifikasikannya ke kluster terdekat dan memperbarui rata-rata klaster. Program akan mengulangi proses untuk beberapa iterasi tetap. Jika antara dua iterasi tidak ada perubahan item klasifikasi, program menghentikan proses sebagai algoritma telah menemukan solusi optimal. Fungsi di bawah ini diambil sebagai input *k* (jumlah klaster yang diinginkan), item dan jumlah iterasi maksimum, dan mengembalikan sarana dan kluster. Klasifikasi item disimpan dalam array milik *To* dan jumlah item dalam sebuah cluster disimpan di *Ukuran cluster* . ```python def CalculateMeans(k,items,maxIterations = 100000 ): # Find the minima and maxima for columns cMin, cMax = FindColMinMax(items); # Initialize means at random points means = InitializeMeans(items,k,cMin,cMax); # Initialize clusters, the array to hold # the number of items in a class clusterSizes = [ 0 for i in range ( len (means))]; # An array to hold the cluster an item is in belongsTo = [ 0 for i in range ( len (items))]; # Calculate means for e in range (maxIterations): # If no change of cluster occurs, halt noChange = True ; for i in range ( len (items)): item = items[i]; # Classify item into a cluster and update the # corresponding means. index = Classify(means,item); clusterSizes[index] + = 1 ; cSize = clusterSizes[index]; means[index] = UpdateMean(cSize,means[index],item); # Item changed cluster if (index ! = belongsTo[i]): noChange = False ; belongsTo[i] = index; # Nothing changed, return if (noChange): break ; return means; ``` Klasifikasi item ke cluster terdekat Program akan mengulangi semua item dan akan mengklasifikasikan setiap item ke kluster terdekat. ```python def FindClusters(means,items): clusters = [[] for i in range ( len (means))]; # Init clusters for item in items: # Classify item into a cluster index = Classify(means,item); # Add item to cluster clusters[index].append(item); clusters; return clusters; ```","title":"K-Means Clustering"},{"location":"K-Means Clustering/#k-means-clustering","text":"","title":"K-Means Clustering"},{"location":"K-Means Clustering/#pendahuluan","text":"K-Means salah satu metode penganalisaan data yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. untuk mencapai ini, kita akan menggunakan algoritma K-Means; algoritma pembelajaran tanpa pengawasan.","title":"Pendahuluan"},{"location":"K-Means Clustering/#algoritma","text":"Algoritma K-Means Clustering akan mengkategorikan item ke dalam kelompok k kesamaan. Untuk menghitung kesamaan itu, akan menggunakan jarak euclidean sebagai pengukuran. Algoritma bekerja sebagai berikut: menentukan jumlah cluster menginisialisasi poin k secara acak ke dalam cluster menghitung rata-rata yang dari setiap cluster mengelompokkan masing-masing data ke dalam rata-rata terdekat mengulangi langkah ketiga sampai tidak ada data yeng berpindah ketika rata-rata selesai dihitung.","title":"Algoritma"},{"location":"K-Means Clustering/#langkah-langkah-untuk-melakukan-k-means-clustering","text":"Langkah-langkah akan dilakukan tanpa mengimport library apapun. jadi pada tugas kali ini, akan melakukan coding murni menggunakan bahasa pemograman Python. Membaca data program menerima input dari file sebagai text ('.txt'). Setiap baris mewakili suatu item, dan itu berisi nilai numerik (satu untuk setiap fitur) dibagi dengan koma. html 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 5.4,3.7,1.5,0.2,Iris-setosa 4.8,3.4,1.6,0.2,Iris-setosa 4.8,3.0,1.4,0.1,Iris-setosa 4.3,3.0,1.1,0.1,Iris-setosa 5.8,4.0,1.2,0.2,Iris-setosa 5.7,4.4,1.5,0.4,Iris-setosa 5.4,3.9,1.3,0.4,Iris-setosa 5.1,3.5,1.4,0.3,Iris-setosa 5.7,3.8,1.7,0.3,Iris-setosa 5.1,3.8,1.5,0.3,Iris-setosa 5.4,3.4,1.7,0.2,Iris-setosa 5.1,3.7,1.5,0.4,Iris-setosa 4.6,3.6,1.0,0.2,Iris-setosa 5.1,3.3,1.7,0.5,Iris-setosa 4.8,3.4,1.9,0.2,Iris-setosa 5.0,3.0,1.6,0.2,Iris-setosa 5.0,3.4,1.6,0.4,Iris-setosa 5.2,3.5,1.5,0.2,Iris-setosa 5.2,3.4,1.4,0.2,Iris-setosa 4.7,3.2,1.6,0.2,Iris-setosa 4.8,3.1,1.6,0.2,Iris-setosa 5.4,3.4,1.5,0.4,Iris-setosa 5.2,4.1,1.5,0.1,Iris-setosa 5.5,4.2,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 5.0,3.2,1.2,0.2,Iris-setosa 5.5,3.5,1.3,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 4.4,3.0,1.3,0.2,Iris-setosa 5.1,3.4,1.5,0.2,Iris-setosa 5.0,3.5,1.3,0.3,Iris-setosa 4.5,2.3,1.3,0.3,Iris-setosa 4.4,3.2,1.3,0.2,Iris-setosa 5.0,3.5,1.6,0.6,Iris-setosa 5.1,3.8,1.9,0.4,Iris-setosa 4.8,3.0,1.4,0.3,Iris-setosa 5.1,3.8,1.6,0.2,Iris-setosa 4.6,3.2,1.4,0.2,Iris-setosa 5.3,3.7,1.5,0.2,Iris-setosa 5.0,3.3,1.4,0.2,Iris-setosa 7.0,3.2,4.7,1.4,Iris-versicolor 6.4,3.2,4.5,1.5,Iris-versicolor 6.9,3.1,4.9,1.5,Iris-versicolor 5.5,2.3,4.0,1.3,Iris-versicolor 6.5,2.8,4.6,1.5,Iris-versicolor 5.7,2.8,4.5,1.3,Iris-versicolor 6.3,3.3,4.7,1.6,Iris-versicolor 4.9,2.4,3.3,1.0,Iris-versicolor 6.6,2.9,4.6,1.3,Iris-versicolor 5.2,2.7,3.9,1.4,Iris-versicolor 5.0,2.0,3.5,1.0,Iris-versicolor 5.9,3.0,4.2,1.5,Iris-versicolor 6.0,2.2,4.0,1.0,Iris-versicolor 6.1,2.9,4.7,1.4,Iris-versicolor 5.6,2.9,3.6,1.3,Iris-versicolor 6.7,3.1,4.4,1.4,Iris-versicolor 5.6,3.0,4.5,1.5,Iris-versicolor 5.8,2.7,4.1,1.0,Iris-versicolor 6.2,2.2,4.5,1.5,Iris-versicolor 5.6,2.5,3.9,1.1,Iris-versicolor 5.9,3.2,4.8,1.8,Iris-versicolor 6.1,2.8,4.0,1.3,Iris-versicolor 6.3,2.5,4.9,1.5,Iris-versicolor 6.1,2.8,4.7,1.2,Iris-versicolor 6.4,2.9,4.3,1.3,Iris-versicolor 6.6,3.0,4.4,1.4,Iris-versicolor 6.8,2.8,4.8,1.4,Iris-versicolor 6.7,3.0,5.0,1.7,Iris-versicolor 6.0,2.9,4.5,1.5,Iris-versicolor 5.7,2.6,3.5,1.0,Iris-versicolor 5.5,2.4,3.8,1.1,Iris-versicolor 5.5,2.4,3.7,1.0,Iris-versicolor 5.8,2.7,3.9,1.2,Iris-versicolor 6.0,2.7,5.1,1.6,Iris-versicolor 5.4,3.0,4.5,1.5,Iris-versicolor 6.0,3.4,4.5,1.6,Iris-versicolor 6.7,3.1,4.7,1.5,Iris-versicolor 6.3,2.3,4.4,1.3,Iris-versicolor 5.6,3.0,4.1,1.3,Iris-versicolor 5.5,2.5,4.0,1.3,Iris-versicolor 5.5,2.6,4.4,1.2,Iris-versicolor 6.1,3.0,4.6,1.4,Iris-versicolor 5.8,2.6,4.0,1.2,Iris-versicolor 5.0,2.3,3.3,1.0,Iris-versicolor 5.6,2.7,4.2,1.3,Iris-versicolor 5.7,3.0,4.2,1.2,Iris-versicolor 5.7,2.9,4.2,1.3,Iris-versicolor 6.2,2.9,4.3,1.3,Iris-versicolor 5.1,2.5,3.0,1.1,Iris-versicolor 5.7,2.8,4.1,1.3,Iris-versicolor 6.3,3.3,6.0,2.5,Iris-virginica 5.8,2.7,5.1,1.9,Iris-virginica 7.1,3.0,5.9,2.1,Iris-virginica 6.3,2.9,5.6,1.8,Iris-virginica 6.5,3.0,5.8,2.2,Iris-virginica 7.6,3.0,6.6,2.1,Iris-virginica 4.9,2.5,4.5,1.7,Iris-virginica 7.3,2.9,6.3,1.8,Iris-virginica 6.7,2.5,5.8,1.8,Iris-virginica 7.2,3.6,6.1,2.5,Iris-virginica 6.5,3.2,5.1,2.0,Iris-virginica 6.4,2.7,5.3,1.9,Iris-virginica 6.8,3.0,5.5,2.1,Iris-virginica 5.7,2.5,5.0,2.0,Iris-virginica 5.8,2.8,5.1,2.4,Iris-virginica 6.4,3.2,5.3,2.3,Iris-virginica 6.5,3.0,5.5,1.8,Iris-virginica 7.7,3.8,6.7,2.2,Iris-virginica 7.7,2.6,6.9,2.3,Iris-virginica 6.0,2.2,5.0,1.5,Iris-virginica 6.9,3.2,5.7,2.3,Iris-virginica 5.6,2.8,4.9,2.0,Iris-virginica 7.7,2.8,6.7,2.0,Iris-virginica 6.3,2.7,4.9,1.8,Iris-virginica 6.7,3.3,5.7,2.1,Iris-virginica 7.2,3.2,6.0,1.8,Iris-virginica 6.2,2.8,4.8,1.8,Iris-virginica 6.1,3.0,4.9,1.8,Iris-virginica 6.4,2.8,5.6,2.1,Iris-virginica 7.2,3.0,5.8,1.6,Iris-virginica 7.4,2.8,6.1,1.9,Iris-virginica 7.9,3.8,6.4,2.0,Iris-virginica 6.4,2.8,5.6,2.2,Iris-virginica 6.3,2.8,5.1,1.5,Iris-virginica 6.1,2.6,5.6,1.4,Iris-virginica 7.7,3.0,6.1,2.3,Iris-virginica 6.3,3.4,5.6,2.4,Iris-virginica 6.4,3.1,5.5,1.8,Iris-virginica 6.0,3.0,4.8,1.8,Iris-virginica 6.9,3.1,5.4,2.1,Iris-virginica 6.7,3.1,5.6,2.4,Iris-virginica 6.9,3.1,5.1,2.3,Iris-virginica 5.8,2.7,5.1,1.9,Iris-virginica 6.8,3.2,5.9,2.3,Iris-virginica 6.7,3.3,5.7,2.5,Iris-virginica 6.7,3.0,5.2,2.3,Iris-virginica 6.3,2.5,5.0,1.9,Iris-virginica 6.5,3.0,5.2,2.0,Iris-virginica 6.2,3.4,5.4,2.3,Iris-virginica 5.9,3.0,5.1,1.8,Iris-virginica Program akan membaca data dari file, menyimpannya ke dalam daftar. Setiap elemen daftar adalah daftar lain yang berisi nilai item untuk fitur. Program melakukan ini dengan fungsi berikut: python def ReadData(fileName): # Read the file, splitting by lines f = open (fileName, 'r' ); lines = f.read().splitlines(); f.close(); items = []; for i in range ( 1 , len (lines)): line = lines[i].split( ',' ); itemFeatures = []; for j in range ( len (line) - 1 ): v = float (line[j]); # Convert feature value to float itemFeatures.append(v); # Add feature value to dict items.append(itemFeatures); shuffle(items); return items; Inisialisasi poin K yang dipilih secara acak Program akan menginisialisasi masing-masing nilai rata-rata dalam kisaran nilai fitur item. Untuk itu, program perlu menemukan min dan maks untuk setiap fitur. Program mencapainya dengan fungsi berikut: python def FindColMinMax(items): n = len (items[ 0 ]); minima = [sys.maxint for i in range (n)]; maxima = [ - sys.maxint - 1 for i in range (n)]; for item in items: for f in range ( len (item)): if (item[f] < minima[f]): minima[f] = item[f]; if (item[f] > maxima[f]): maxima[f] = item[f]; return minima,maxima; Variabel minima, maxima adalah daftar yang berisi nilai min dan max masing-masing item. Program menginisialisasi setiap nilai fitur rata-rata secara acak antara minimum dan maksimum yang sesuai pada dua daftar di atas: python def InitializeMeans(items, k, cMin, cMax): # Initialize means to random numbers between # the min and max of each column/feature f = len (items[ 0 ]); # number of features means = [[ 0 for i in range (f)] for j in range (k)]; for mean in means: for i in range ( len (mean)): # Set value to a random float # (adding +-1 to avoid a wide placement of a mean) mean[i] = uniform(cMin[i] + 1 , cMax[i] - 1 ); return means; Menghitung jarak Euclidean Program akan menggunakan jarak euclidean sebagai metrik kesamaan untuk set data kami (catatan: tergantung pada item Anda, Anda dapat menggunakan metrik kesamaan lainnya). ```python def EuclideanDistance(x, y): S = 0 ; # The sum of the squared differences of the elements for i in range ( len (x)): S + = math. pow (x[i] - y[i], 2 ); return math.sqrt(S); #The square root of the sum ``` Menghitung rata-rata ```python def UpdateMean(n,mean,item): for i in range ( len (mean)): m = mean[i]; m = (m * (n - 1 ) + item[i]) / float (n); mean[i] = round (m, 3 ); return mean; ``` Klasifikasi Item ```python def Classify(means,item): # Classify item to the mean with minimum distance minimum = sys.maxint; index = - 1 ; for i in range ( len (means)): # Find distance from item to mean dis = EuclideanDistance(item, means[i]); if (dis < minimum): minimum = dis; index = i; index; return index; ``` Menghitung kembali rata-rata cluster Untuk benar-benar menemukan rata-rata, program akan mengulang semua item, mengklasifikasikannya ke kluster terdekat dan memperbarui rata-rata klaster. Program akan mengulangi proses untuk beberapa iterasi tetap. Jika antara dua iterasi tidak ada perubahan item klasifikasi, program menghentikan proses sebagai algoritma telah menemukan solusi optimal. Fungsi di bawah ini diambil sebagai input *k* (jumlah klaster yang diinginkan), item dan jumlah iterasi maksimum, dan mengembalikan sarana dan kluster. Klasifikasi item disimpan dalam array milik *To* dan jumlah item dalam sebuah cluster disimpan di *Ukuran cluster* . ```python def CalculateMeans(k,items,maxIterations = 100000 ): # Find the minima and maxima for columns cMin, cMax = FindColMinMax(items); # Initialize means at random points means = InitializeMeans(items,k,cMin,cMax); # Initialize clusters, the array to hold # the number of items in a class clusterSizes = [ 0 for i in range ( len (means))]; # An array to hold the cluster an item is in belongsTo = [ 0 for i in range ( len (items))]; # Calculate means for e in range (maxIterations): # If no change of cluster occurs, halt noChange = True ; for i in range ( len (items)): item = items[i]; # Classify item into a cluster and update the # corresponding means. index = Classify(means,item); clusterSizes[index] + = 1 ; cSize = clusterSizes[index]; means[index] = UpdateMean(cSize,means[index],item); # Item changed cluster if (index ! = belongsTo[i]): noChange = False ; belongsTo[i] = index; # Nothing changed, return if (noChange): break ; return means; ``` Klasifikasi item ke cluster terdekat Program akan mengulangi semua item dan akan mengklasifikasikan setiap item ke kluster terdekat. ```python def FindClusters(means,items): clusters = [[] for i in range ( len (means))]; # Init clusters for item in items: # Classify item into a cluster index = Classify(means,item); # Add item to cluster clusters[index].append(item); clusters; return clusters; ```","title":"Langkah-langkah untuk melakukan K-Means Clustering"},{"location":"Klasifikasi/","text":"DECISION TREES \u00b6 Classification \u00b6 Decision tree membangun model klasifikasi atau regresi dalam bentuk struktur pohon. Ini memecah dataset menjadi himpunan bagian yang lebih kecil dan lebih kecil sementara pada saat yang sama pohon keputusan terkait dikembangkan secara bertahap. Hasil akhirnya adalah pohon dengan simpul keputusan dan simpul daun. Node keputusan (mis., Outlook) memiliki dua atau lebih cabang (mis., Sunny, Overcast, dan Rainy). Node daun (mis., Play) mewakili klasifikasi atau keputusan. Node keputusan teratas dalam pohon yang sesuai dengan prediktor terbaik disebut simpul akar. Pohon keputusan dapat menangani data kategorikal dan numerik. Decision Tree terdiri dari: \u00b6 Node: Tes untuk nilai atribut tertentu. Edges / Branch Sesuai dengan hasil tes dan terhubung ke simpul atau daun berikutnya. Leaf Node: Node terminal yang memprediksi hasil (mewakili label kelas atau distribusi kelas). Algorithm \u00b6 Algoritma inti untuk membangun pohon keputusan yang disebut ID3 oleh J. R. Quinlan yang menggunakan pencarian serakah top-down melalui ruang cabang yang mungkin tanpa backtracking. ID3 menggunakan Entropi dan Penguatan Informasi untuk membangun pohon keputusan. Dalam model ZeroR tidak ada prediktor, dalam model OneR kami mencoba menemukan prediktor tunggal terbaik, Bayesian naif mencakup semua prediktor yang menggunakan aturan Bayes dan asumsi independensi antara prediktor, tetapi pohon keputusan mencakup semua prediktor dengan asumsi ketergantungan antara prediktor. Entropy \u00b6 Pohon keputusan dibangun dari atas ke bawah dari simpul akar dan melibatkan mempartisi data ke dalam himpunan bagian yang berisi instance dengan nilai yang sama (homogen). Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi rata, entropinya satu. $$ Entropy(S) = \\sum_{i=1}^n {-P_i\\log_2{P_i}} $$ Keterangan : S = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Pi = Probability dari Pi terhadap P Information Gain \u00b6 Gain informasi didasarkan pada penurunan entropi setelah dataset dibagi pada atribut. Membangun pohon keputusan adalah tentang menemukan atribut yang mengembalikan perolehan informasi tertinggi (mis., Cabang yang paling homogen). $$ Gain(S,A) = entropy(S)-\\sum_{i=1}^n \\frac{|s_i|}{|s|}\\quad x \\quad entropy(S_i) $$ Keterangan : T = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Si = Probability dari Si terhadap S Data Berikut untuk menghitung Entropy,Gain yang ada di tiap data tersebut menggunakan hitugan manual : \u00b6 Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Kita harus menghitung Entropy terlebih dahulu dengan menggunakan rumus Entropy(S) $$ P_i = C0 = , P(C0)=P_1 = \\frac{10}{20} $$ $$ P_i = C0 = , P(C1)=P_2 = \\frac{10}{20} $$ $$ Entropy(S) = {-\\frac{10}{20}\\log_2{\\frac{10}{20}}}{-\\frac{10}{20}\\log_2{\\frac{10}{20}}}= 1 $$ Sudah dicari Entropynya = 1 maka selanjutnya mencari tiap Gainnya Gain dari Gender dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari CarType dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari Shirt Size dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value $$ Entropy(Male) = {-\\frac{6}{10}\\log_2{\\frac{6}{10}}}{-\\frac{4}{10}\\log_2{\\frac{4}{10}}}=0,97051 $$ $$ Entropy(Female) = {-\\frac{4}{10}\\log_2{\\frac{4}{10}}}{-\\frac{6}{10}\\log_2{\\frac{6}{10}}}=0,97051 $$ $$ Entropy(Family) = {-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}}=0,81128 $$ $$ Entropy(Sports) = {-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}=0 $$ $$ Entropy(Luxury) = {-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}=0,54356 $$ $$ Gain(S,CarType) = 1-(\\frac{4}{20}X({-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}})+\\frac{8}{20}X({-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}+\\frac{8}{20}X({-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}) = 0,62032 $$ $$ Entropy(Small) = {-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}}=0,97095 $$ $$ Entropy(Medium) = {-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}=0,98523 $$ $$ Entropy(Large) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Entropy(ExtraLarge) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Gain(S,ShirtSize) = 1-(\\frac{5}{20}X({-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}})+\\frac{7}{20}X({-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}) = 0,012433 $$ Program untuk menghitung entropy, gain dengan program: import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.tree import DecisionTreeClassifier from sklearn import tree data = pd . read_excel ( \"data_informationgain.xlsx\" ) df = pd . DataFrame ( data ) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Merubah label / kolom yang ada di data diatas untuk bisa dihitung dengan menggunakan code python,yaitu untuk memudahkan pengguna agar bisa menghitung code yang nanti akan digunakan . Pada gender hanya menggunakan biner (0,1){F,M}. Pada cartype menggunakan inisialisasi angka urutan (0,1,2){Familiy,Luxury,Sports} . Pada Shirt Size menggunakan inisialisasi angka urutan (0,1,2,3){Small,Medium,Large,ExtraLarge}.Berikut codenya lab = LabelEncoder () df [ \"gender_n\" ] = lab . fit_transform ( df [ \"Gender\" ]) df [ \"car_type_n\" ] = lab . fit_transform ( df [ \"Car Type\" ]) df [ \"shirt_size_n\" ] = lab . fit_transform ( df [ \"Shirt Size\" ]) df [ \"class_n\" ] = lab . fit_transform ( df [ \"Class\" ]) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class gender_n car_type_n shirt_size_n class_n 1 M Family Small C0 1 0 3 0 2 M Sports Medium C0 1 2 2 0 3 M Sports Medium C0 1 2 2 0 4 M Sports Large C0 1 2 1 0 5 M Sports Extra Large C0 1 2 0 0 6 M Sports Extra Large C0 1 2 0 0 7 F Sports Small C0 0 2 3 0 8 F Sports Small C0 0 2 3 0 9 F Sports Medium C0 0 2 2 0 10 F Luxury Large C0 0 1 1 0 11 M Family Large C1 1 0 1 1 12 M Family Extra Large C1 1 0 0 1 13 M Family Medium C1 1 0 2 1 14 M Luxury Extra Large C1 1 1 0 1 15 F Luxury Small C1 0 1 3 1 16 F Luxury Small C1 0 1 3 1 17 F Luxury Medium C1 0 1 2 1 18 F Luxury Medium C1 0 1 2 1 19 F Luxury Medium C1 0 1 2 1 20 F Luxury Large C1 0 1 Setelah merubah label menjadi inisialisasi angka (numerik) melanjutkan ke code berikutnya untuk bisa dihitung dan code berikut untuk penghapusan fitur /label yang tidak diperlukan inputs = df . drop ([ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" , \"Class\" , \"class_n\" ], axis = \"columns\" ) target = df [ \"class_n\" ] Membuat Klasifikasi fitur untuk bisa dibuat Pohon Keputusan (Decision Tree)untuk bisa di classifier . model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best') from matplotlib import pyplot as plt tree . plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" ], class_names = [ \"C0\" , \"C1\" ], label = \"all\" , filled = True , impurity = True , node_ids = True , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"DECISION TREES"},{"location":"Klasifikasi/#decision-trees","text":"","title":"DECISION TREES"},{"location":"Klasifikasi/#classification","text":"Decision tree membangun model klasifikasi atau regresi dalam bentuk struktur pohon. Ini memecah dataset menjadi himpunan bagian yang lebih kecil dan lebih kecil sementara pada saat yang sama pohon keputusan terkait dikembangkan secara bertahap. Hasil akhirnya adalah pohon dengan simpul keputusan dan simpul daun. Node keputusan (mis., Outlook) memiliki dua atau lebih cabang (mis., Sunny, Overcast, dan Rainy). Node daun (mis., Play) mewakili klasifikasi atau keputusan. Node keputusan teratas dalam pohon yang sesuai dengan prediktor terbaik disebut simpul akar. Pohon keputusan dapat menangani data kategorikal dan numerik.","title":"Classification"},{"location":"Klasifikasi/#decision-tree-terdiri-dari","text":"Node: Tes untuk nilai atribut tertentu. Edges / Branch Sesuai dengan hasil tes dan terhubung ke simpul atau daun berikutnya. Leaf Node: Node terminal yang memprediksi hasil (mewakili label kelas atau distribusi kelas).","title":"Decision Tree terdiri dari:"},{"location":"Klasifikasi/#algorithm","text":"Algoritma inti untuk membangun pohon keputusan yang disebut ID3 oleh J. R. Quinlan yang menggunakan pencarian serakah top-down melalui ruang cabang yang mungkin tanpa backtracking. ID3 menggunakan Entropi dan Penguatan Informasi untuk membangun pohon keputusan. Dalam model ZeroR tidak ada prediktor, dalam model OneR kami mencoba menemukan prediktor tunggal terbaik, Bayesian naif mencakup semua prediktor yang menggunakan aturan Bayes dan asumsi independensi antara prediktor, tetapi pohon keputusan mencakup semua prediktor dengan asumsi ketergantungan antara prediktor.","title":"Algorithm"},{"location":"Klasifikasi/#entropy","text":"Pohon keputusan dibangun dari atas ke bawah dari simpul akar dan melibatkan mempartisi data ke dalam himpunan bagian yang berisi instance dengan nilai yang sama (homogen). Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi rata, entropinya satu. $$ Entropy(S) = \\sum_{i=1}^n {-P_i\\log_2{P_i}} $$ Keterangan : S = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Pi = Probability dari Pi terhadap P","title":"Entropy"},{"location":"Klasifikasi/#information-gain","text":"Gain informasi didasarkan pada penurunan entropi setelah dataset dibagi pada atribut. Membangun pohon keputusan adalah tentang menemukan atribut yang mengembalikan perolehan informasi tertinggi (mis., Cabang yang paling homogen). $$ Gain(S,A) = entropy(S)-\\sum_{i=1}^n \\frac{|s_i|}{|s|}\\quad x \\quad entropy(S_i) $$ Keterangan : T = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Si = Probability dari Si terhadap S","title":"Information Gain"},{"location":"Klasifikasi/#data-berikut-untuk-menghitung-entropygain-yang-ada-di-tiap-data-tersebut-menggunakan-hitugan-manual","text":"Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Kita harus menghitung Entropy terlebih dahulu dengan menggunakan rumus Entropy(S) $$ P_i = C0 = , P(C0)=P_1 = \\frac{10}{20} $$ $$ P_i = C0 = , P(C1)=P_2 = \\frac{10}{20} $$ $$ Entropy(S) = {-\\frac{10}{20}\\log_2{\\frac{10}{20}}}{-\\frac{10}{20}\\log_2{\\frac{10}{20}}}= 1 $$ Sudah dicari Entropynya = 1 maka selanjutnya mencari tiap Gainnya Gain dari Gender dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari CarType dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value Gain dari Shirt Size dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value $$ Entropy(Male) = {-\\frac{6}{10}\\log_2{\\frac{6}{10}}}{-\\frac{4}{10}\\log_2{\\frac{4}{10}}}=0,97051 $$ $$ Entropy(Female) = {-\\frac{4}{10}\\log_2{\\frac{4}{10}}}{-\\frac{6}{10}\\log_2{\\frac{6}{10}}}=0,97051 $$ $$ Entropy(Family) = {-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}}=0,81128 $$ $$ Entropy(Sports) = {-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}=0 $$ $$ Entropy(Luxury) = {-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}=0,54356 $$ $$ Gain(S,CarType) = 1-(\\frac{4}{20}X({-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}})+\\frac{8}{20}X({-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}+\\frac{8}{20}X({-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}) = 0,62032 $$ $$ Entropy(Small) = {-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}}=0,97095 $$ $$ Entropy(Medium) = {-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}=0,98523 $$ $$ Entropy(Large) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Entropy(ExtraLarge) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Gain(S,ShirtSize) = 1-(\\frac{5}{20}X({-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}})+\\frac{7}{20}X({-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}) = 0,012433 $$ Program untuk menghitung entropy, gain dengan program: import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.tree import DecisionTreeClassifier from sklearn import tree data = pd . read_excel ( \"data_informationgain.xlsx\" ) df = pd . DataFrame ( data ) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Merubah label / kolom yang ada di data diatas untuk bisa dihitung dengan menggunakan code python,yaitu untuk memudahkan pengguna agar bisa menghitung code yang nanti akan digunakan . Pada gender hanya menggunakan biner (0,1){F,M}. Pada cartype menggunakan inisialisasi angka urutan (0,1,2){Familiy,Luxury,Sports} . Pada Shirt Size menggunakan inisialisasi angka urutan (0,1,2,3){Small,Medium,Large,ExtraLarge}.Berikut codenya lab = LabelEncoder () df [ \"gender_n\" ] = lab . fit_transform ( df [ \"Gender\" ]) df [ \"car_type_n\" ] = lab . fit_transform ( df [ \"Car Type\" ]) df [ \"shirt_size_n\" ] = lab . fit_transform ( df [ \"Shirt Size\" ]) df [ \"class_n\" ] = lab . fit_transform ( df [ \"Class\" ]) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class gender_n car_type_n shirt_size_n class_n 1 M Family Small C0 1 0 3 0 2 M Sports Medium C0 1 2 2 0 3 M Sports Medium C0 1 2 2 0 4 M Sports Large C0 1 2 1 0 5 M Sports Extra Large C0 1 2 0 0 6 M Sports Extra Large C0 1 2 0 0 7 F Sports Small C0 0 2 3 0 8 F Sports Small C0 0 2 3 0 9 F Sports Medium C0 0 2 2 0 10 F Luxury Large C0 0 1 1 0 11 M Family Large C1 1 0 1 1 12 M Family Extra Large C1 1 0 0 1 13 M Family Medium C1 1 0 2 1 14 M Luxury Extra Large C1 1 1 0 1 15 F Luxury Small C1 0 1 3 1 16 F Luxury Small C1 0 1 3 1 17 F Luxury Medium C1 0 1 2 1 18 F Luxury Medium C1 0 1 2 1 19 F Luxury Medium C1 0 1 2 1 20 F Luxury Large C1 0 1 Setelah merubah label menjadi inisialisasi angka (numerik) melanjutkan ke code berikutnya untuk bisa dihitung dan code berikut untuk penghapusan fitur /label yang tidak diperlukan inputs = df . drop ([ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" , \"Class\" , \"class_n\" ], axis = \"columns\" ) target = df [ \"class_n\" ] Membuat Klasifikasi fitur untuk bisa dibuat Pohon Keputusan (Decision Tree)untuk bisa di classifier . model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best') from matplotlib import pyplot as plt tree . plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" ], class_names = [ \"C0\" , \"C1\" ], label = \"all\" , filled = True , impurity = True , node_ids = True , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Data Berikut untuk menghitung Entropy,Gain yang ada di tiap data tersebut menggunakan hitugan manual :"},{"location":"Missing Value/","text":"Mencari Data Hilang Menggunakan K-Nearest Neighbour \u00b6 salah satu permasalahan yang sering terjadi adalah adanya data yang hilang dalam suatu database. Metode yang paling mudah untuk menyelesaikan masalah ini adalah menggunakan Knn atau K-Nearest Neighbour. Kelemahan metode ini adalah pemilihan nilai k yang tidak tepat dapat menurunkan kinerja klasifikasi. Metode ini juga merupakan metode yang sederhana dan fleksibel dikarenakan dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit. langkah penting dalam knn adalah menentukan nilai k dimana sejumlah k tetangga terdekat dari dataset akan dijadikan sebagai nilai estimator. Pemilihan sejumlah tetangga didasarkan pada jarak salah satu instance dengan seluruh instance yang ada pada dataset. Dari informasi tetangga tersebut diperoleh estimasi nilai yang kemudian digunakan sebagai nilai imputasi pada data yang hilang. Perhitungan dalam menentukan nilai imputasi tergantung pada jenis data, untuk data kontinu digunakan rata-rata dari tetangga terdekat, sedangkan untuk data kualitatif nilai imputasi diambil dari nilai yang seringkali keluar. Algoritma KNN \u00b6 Tentukan nilai k Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan rata-rata k tetangga terdekat di Dc # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 95 , 70 , np . nan , 90 ], 'Second Score' : [ 75 , 70 , 91 , np . nan ], 'Third Score' :[ np . nan , 75 , 85 , 90 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); First Score Second Score Third Score 0 95.0 75.0 0.0 1 70.0 70.0 75.0 2 0.0 91.0 85.0 3 90.0 0.0 90.0","title":"Missing Value"},{"location":"Missing Value/#mencari-data-hilang-menggunakan-k-nearest-neighbour","text":"salah satu permasalahan yang sering terjadi adalah adanya data yang hilang dalam suatu database. Metode yang paling mudah untuk menyelesaikan masalah ini adalah menggunakan Knn atau K-Nearest Neighbour. Kelemahan metode ini adalah pemilihan nilai k yang tidak tepat dapat menurunkan kinerja klasifikasi. Metode ini juga merupakan metode yang sederhana dan fleksibel dikarenakan dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit. langkah penting dalam knn adalah menentukan nilai k dimana sejumlah k tetangga terdekat dari dataset akan dijadikan sebagai nilai estimator. Pemilihan sejumlah tetangga didasarkan pada jarak salah satu instance dengan seluruh instance yang ada pada dataset. Dari informasi tetangga tersebut diperoleh estimasi nilai yang kemudian digunakan sebagai nilai imputasi pada data yang hilang. Perhitungan dalam menentukan nilai imputasi tergantung pada jenis data, untuk data kontinu digunakan rata-rata dari tetangga terdekat, sedangkan untuk data kualitatif nilai imputasi diambil dari nilai yang seringkali keluar.","title":"Mencari Data Hilang Menggunakan K-Nearest Neighbour"},{"location":"Missing Value/#algoritma-knn","text":"Tentukan nilai k Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan rata-rata k tetangga terdekat di Dc # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 95 , 70 , np . nan , 90 ], 'Second Score' : [ 75 , 70 , 91 , np . nan ], 'Third Score' :[ np . nan , 75 , 85 , 90 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); First Score Second Score Third Score 0 95.0 75.0 0.0 1 70.0 70.0 75.0 2 0.0 91.0 85.0 3 90.0 0.0 90.0","title":"Algoritma KNN"},{"location":"Statistik Dekriptif/","text":"STATISTIK DESKRIPTIF \u00b6 1. PENGERTIAN Statistik deskritif memberikan informasi tentang data yang dipunyai dan sama sekali tidak menarik inferensia atau kesimpulan apapun tentang gugus induknya yang lebih besar. Dengan statistik ini, kumpulan data yang diperoleh akan tersaji denga ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada. 2. UKURAN TENDENSI SENTRAL Ukuran tendensi sentral adalah setiap pengukuran aritmatika yang ditujukan untuk menggambarkan suatu nilai yang mewakili nilai pusat atau nilai sentral dari suatu gugus data (himpunan pengamatan) berikut adalah bebrapa jenis ukuran tendensi sentral yang sering digunakan: a. Mean Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Dimana: x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyaknya data b. Modus Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana: Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak (selalu positif) c. Median Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut . Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Median dari kelompok data n = banyak data d. Varians Varians adalah suatu ukuran penyebaran data, yang diukur dalam pangkat dua dari selisih data terhadap rata-ratanya. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); e. Standard Deviasi Standar deviasi merupakan akar dari varians (ingat, karena pada varians kita mengkuadratkan selisih data dari rata-ratanya, maka dengan mengakarkannya, kita mendapatkan kembali nilai asalnya). 3. ALAT DAN BAHAN pada tugas kali ini saya menggunakan data random sebanyak 500 data yang disimpan dalam bentuk .csv. Saya juga menggunakan IDLE Python dan Library pandas, matplotlib juga scipy. pandas berguna untuk membaca file .csv dan untuk mengolah data. Mtpolotlib berguna untuk memvisualisasikan data dengan lebih indah dan rapi. Sedangkan scipy berguna untuk menangani operasi aljabar dan matriks serta operasi matematika lainnya. a. Langkah Pertama Memasukkan library from scipy import stats import pandas as pd import matplotlib.pyplot as plt b. Langkah Kedua mengimport data .csv yang telah disipakan df = pd . read_csv ( 'tugas1.csv' , sep = ';' ) df c. Langkah Ketiga Menampung nilai yang akan ditampilkan didalam data penyimpanan (dictionary). Membuat iterasi yang akan mengambil data pada file yangtelah diimport. data = { \"stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard deviasi' , 'Variasi' , 'Skewnes' , 'Q1' , 'Q2' , 'Q3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'stats' ] + [ x for x in df . columns ]) tes d. Langkah Keempat memvisualisasikan hasil tersebut dalam bentuk dataframe data = { \"stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard deviasi' , 'Variasi' , 'Skewnes' , 'Q1' , 'Q2' , 'Q3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'stats' ] + [ x for x in df . columns ]) tes stats x1 x2 x3 x4 0 Min 60.000 60.00 60.000 60.000 1 Max 100.000 100.00 100.000 100.000 2 Mean 79.478 80.29 80.334 80.256 3 Standard deviasi 12.080 11.25 11.540 11.790 4 Variasi 145.990 126.66 133.260 139.080 5 Skewnes 0.060 -0.06 0.010 -0.000 6 Q1 69.000 71.00 71.000 70.000 7 Q2 78.500 81.00 80.500 80.000 8 Q3 90.000 89.00 90.000 90.000 9 Median 78.500 81.00 80.500 80.000 10 Modus 72.000 86.00 71.000 93.000","title":"statistik deskriptif"},{"location":"Statistik Dekriptif/#statistik-deskriptif","text":"1. PENGERTIAN Statistik deskritif memberikan informasi tentang data yang dipunyai dan sama sekali tidak menarik inferensia atau kesimpulan apapun tentang gugus induknya yang lebih besar. Dengan statistik ini, kumpulan data yang diperoleh akan tersaji denga ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada. 2. UKURAN TENDENSI SENTRAL Ukuran tendensi sentral adalah setiap pengukuran aritmatika yang ditujukan untuk menggambarkan suatu nilai yang mewakili nilai pusat atau nilai sentral dari suatu gugus data (himpunan pengamatan) berikut adalah bebrapa jenis ukuran tendensi sentral yang sering digunakan: a. Mean Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Dimana: x bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyaknya data b. Modus Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Dimana: Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemet sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen sesudahnya p = panjang interval nilai b1 dan b2 \u2013> adalah mutlak (selalu positif) c. Median Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut . Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Dimana : Me = Median dari kelompok data n = banyak data d. Varians Varians adalah suatu ukuran penyebaran data, yang diukur dalam pangkat dua dari selisih data terhadap rata-ratanya. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); e. Standard Deviasi Standar deviasi merupakan akar dari varians (ingat, karena pada varians kita mengkuadratkan selisih data dari rata-ratanya, maka dengan mengakarkannya, kita mendapatkan kembali nilai asalnya). 3. ALAT DAN BAHAN pada tugas kali ini saya menggunakan data random sebanyak 500 data yang disimpan dalam bentuk .csv. Saya juga menggunakan IDLE Python dan Library pandas, matplotlib juga scipy. pandas berguna untuk membaca file .csv dan untuk mengolah data. Mtpolotlib berguna untuk memvisualisasikan data dengan lebih indah dan rapi. Sedangkan scipy berguna untuk menangani operasi aljabar dan matriks serta operasi matematika lainnya. a. Langkah Pertama Memasukkan library from scipy import stats import pandas as pd import matplotlib.pyplot as plt b. Langkah Kedua mengimport data .csv yang telah disipakan df = pd . read_csv ( 'tugas1.csv' , sep = ';' ) df c. Langkah Ketiga Menampung nilai yang akan ditampilkan didalam data penyimpanan (dictionary). Membuat iterasi yang akan mengambil data pada file yangtelah diimport. data = { \"stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard deviasi' , 'Variasi' , 'Skewnes' , 'Q1' , 'Q2' , 'Q3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'stats' ] + [ x for x in df . columns ]) tes d. Langkah Keempat memvisualisasikan hasil tersebut dalam bentuk dataframe data = { \"stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standard deviasi' , 'Variasi' , 'Skewnes' , 'Q1' , 'Q2' , 'Q3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'stats' ] + [ x for x in df . columns ]) tes stats x1 x2 x3 x4 0 Min 60.000 60.00 60.000 60.000 1 Max 100.000 100.00 100.000 100.000 2 Mean 79.478 80.29 80.334 80.256 3 Standard deviasi 12.080 11.25 11.540 11.790 4 Variasi 145.990 126.66 133.260 139.080 5 Skewnes 0.060 -0.06 0.010 -0.000 6 Q1 69.000 71.00 71.000 70.000 7 Q2 78.500 81.00 80.500 80.000 8 Q3 90.000 89.00 90.000 90.000 9 Median 78.500 81.00 80.500 80.000 10 Modus 72.000 86.00 71.000 93.000","title":"STATISTIK DESKRIPTIF"},{"location":"jarak data/","text":"Mengukur Jarak Data \u00b6 Komponen utama dalam algoritma clustering berbasis jarak adalah mengukur jarak antar data. Untuk Melakukan suat pengelompokan algoritma yang dipakai adalah algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering tergantung pada jaraknya. Ukuran kesamaan adalah ukuran seberapa mirip dua objek data. Ukuran kesamaan dalam konteks data mining adalah jarak dengan dimensi yang mewakili fitur dari objek. Jika jarak ini kecil, itu akan menjadi tingkat kesamaan yang tinggi di mana jarak yang besar akan menjadi tingkat kesamaan yang rendah. Kesamaannya adalah subyektif dan sangat tergantung pada domain dan aplikasi. Misalnya, dua buah serupa karena warna atau ukuran atau rasa. Perhatian harus diambil saat menghitung jarak lintas dimensi / fitur yang tidak terkait. Nilai-nilai relatif dari setiap elemen harus dinormalisasi, atau satu fitur bisa berakhir mendominasi perhitungan jarak. Manhattan Distance \u00b6 Mahattan Distance sensitif terhadap outlier. Rumus Mahattan cenderung lebih cepat dibandingkan dengan rumus Euclidean namun dari beberapa percobaan hasil proses clustering Euclidean lebih baik dari Manhattan. $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ Contoh Implementasi Program Python Menggunakan Manhattan Distance \u00b6 from math import * def manhattan_distance ( x , y ): return sum ( abs ( a - b ) for a , b in zip ( x , y )) print manhattan_distance ([ 10 , 20 , 10 ],[ 10 , 20 , 20 ]) 10 [ Finished in 0.0 s ] Euclidean Distance \u00b6 metode ini merupakan metode paling terkenal. untuk mempelajari hubungan antara sudut dan jarak. Euclidean distance adalah metode yang lebih baik dari pada Mahattan Distance. Jarak Euclidean adalah penggunaan jarak yang paling umum. Dalam kebanyakan kasus ketika orang mengatakan tentang jarak, mereka akan merujuk pada jarak Euclidean. Jarak Euclidean juga dikenal sebagai jarak sederhana. Ketika data padat atau kontinu, ini adalah ukuran kedekatan terbaik. Jarak Euclidean antara dua titik adalah panjang jalur yang menghubungkannya. Teorema Pythagoras memberikan jarak ini antara dua titik. $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); Contoh Implementasi Euclidean Distance pada python \u00b6 from math import * def euclidean_distance ( x , y ): return sqrt ( sum ( pow ( a - b , 2 ) for a , b in zip ( x , y ))) print euclidean_distance ([ 0 , 3 , 4 , 5 ],[ 7 , 6 , 3 , - 1 ]) 9.74679434481 [ Finished in 0.0 s ] Manhattan distance implementation in python: \u00b6 def manhattan_distance ( x , y ): return sum ( abs ( a - b ) for a , b in zip ( x , y )) print manhattan_distance ([ 10 , 20 , 10 ],[ 10 , 20 , 20 ]) 10 [ Finished in 0.0 s ] Minkowski Distance \u00b6 Jarak Minkowski adalah bentuk metrik umum jarak Euclidean dan jarak Manhattan. Cara jarak diukur dengan metrik Minkowski dengan urutan berbeda antara dua objek dengan tiga variabel (Pada gambar ditampilkan dalam sistem koordinat dengan sumbu x, y, z). $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ Minkowski distance implementation in python: \u00b6 from math import * from decimal import Decimal def nth_root ( value , n_root ): root_value = 1 / float ( n_root ) return round ( Decimal ( value ) ** Decimal ( root_value ), 3 ) def minkowski_distance ( x , y , p_value ): return nth_root ( sum ( pow ( abs ( a - b ), p_value ) for a , b in zip ( x , y )), p_value ) print minkowski_distance ([ 0 , 3 , 4 , 5 ],[ 7 , 6 , 3 , - 1 ], 3 )","title":"Jarak Data"},{"location":"jarak data/#mengukur-jarak-data","text":"Komponen utama dalam algoritma clustering berbasis jarak adalah mengukur jarak antar data. Untuk Melakukan suat pengelompokan algoritma yang dipakai adalah algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering tergantung pada jaraknya. Ukuran kesamaan adalah ukuran seberapa mirip dua objek data. Ukuran kesamaan dalam konteks data mining adalah jarak dengan dimensi yang mewakili fitur dari objek. Jika jarak ini kecil, itu akan menjadi tingkat kesamaan yang tinggi di mana jarak yang besar akan menjadi tingkat kesamaan yang rendah. Kesamaannya adalah subyektif dan sangat tergantung pada domain dan aplikasi. Misalnya, dua buah serupa karena warna atau ukuran atau rasa. Perhatian harus diambil saat menghitung jarak lintas dimensi / fitur yang tidak terkait. Nilai-nilai relatif dari setiap elemen harus dinormalisasi, atau satu fitur bisa berakhir mendominasi perhitungan jarak.","title":"Mengukur Jarak Data"},{"location":"jarak data/#manhattan-distance","text":"Mahattan Distance sensitif terhadap outlier. Rumus Mahattan cenderung lebih cepat dibandingkan dengan rumus Euclidean namun dari beberapa percobaan hasil proses clustering Euclidean lebih baik dari Manhattan. $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$","title":"Manhattan Distance"},{"location":"jarak data/#contoh-implementasi-program-python-menggunakan-manhattan-distance","text":"from math import * def manhattan_distance ( x , y ): return sum ( abs ( a - b ) for a , b in zip ( x , y )) print manhattan_distance ([ 10 , 20 , 10 ],[ 10 , 20 , 20 ]) 10 [ Finished in 0.0 s ]","title":"Contoh Implementasi Program Python Menggunakan Manhattan Distance"},{"location":"jarak data/#euclidean-distance","text":"metode ini merupakan metode paling terkenal. untuk mempelajari hubungan antara sudut dan jarak. Euclidean distance adalah metode yang lebih baik dari pada Mahattan Distance. Jarak Euclidean adalah penggunaan jarak yang paling umum. Dalam kebanyakan kasus ketika orang mengatakan tentang jarak, mereka akan merujuk pada jarak Euclidean. Jarak Euclidean juga dikenal sebagai jarak sederhana. Ketika data padat atau kontinu, ini adalah ukuran kedekatan terbaik. Jarak Euclidean antara dua titik adalah panjang jalur yang menghubungkannya. Teorema Pythagoras memberikan jarak ini antara dua titik. $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Euclidean Distance"},{"location":"jarak data/#contoh-implementasi-euclidean-distance-pada-python","text":"from math import * def euclidean_distance ( x , y ): return sqrt ( sum ( pow ( a - b , 2 ) for a , b in zip ( x , y ))) print euclidean_distance ([ 0 , 3 , 4 , 5 ],[ 7 , 6 , 3 , - 1 ]) 9.74679434481 [ Finished in 0.0 s ]","title":"Contoh Implementasi Euclidean Distance pada python"},{"location":"jarak data/#manhattan-distance-implementation-in-python","text":"def manhattan_distance ( x , y ): return sum ( abs ( a - b ) for a , b in zip ( x , y )) print manhattan_distance ([ 10 , 20 , 10 ],[ 10 , 20 , 20 ]) 10 [ Finished in 0.0 s ]","title":"Manhattan distance implementation in python:"},{"location":"jarak data/#minkowski-distance","text":"Jarak Minkowski adalah bentuk metrik umum jarak Euclidean dan jarak Manhattan. Cara jarak diukur dengan metrik Minkowski dengan urutan berbeda antara dua objek dengan tiga variabel (Pada gambar ditampilkan dalam sistem koordinat dengan sumbu x, y, z). $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$","title":"Minkowski Distance"},{"location":"jarak data/#minkowski-distance-implementation-in-python","text":"from math import * from decimal import Decimal def nth_root ( value , n_root ): root_value = 1 / float ( n_root ) return round ( Decimal ( value ) ** Decimal ( root_value ), 3 ) def minkowski_distance ( x , y , p_value ): return nth_root ( sum ( pow ( abs ( a - b ), p_value ) for a , b in zip ( x , y )), p_value ) print minkowski_distance ([ 0 , 3 , 4 , 5 ],[ 7 , 6 , 3 , - 1 ], 3 )","title":"Minkowski distance implementation in python:"},{"location":"profil/","text":"Nama : Nadia Asri NIM : 180411100063 Kelas : Komputasi Numerik B","title":"index"},{"location":"regresi/","text":"Regresi Linear Sederhana dan Regresi Linear Berganda \u00b6 PENGERTIAN Regresi secara umum adalah studi mengenai ketergantungan suatu variabel dependen (terikat) dengan satu atau lebih variabel independent (variabel penjelas/bebas). Tujuannya adalah untuk mengestimasi dan/atau memprediksi rata-rata populasi atau niiai rata-rata variabel dependen berdasarkan nilai variabel independen yang diketahui. Pusat perhatian adalah pada upaya menjelaskan dan mengevalusihubungan antara suatu variabel dengan satu atau lebih variabel independen. Hasil analisis regresi adalah berupa koefisien regresi untuk masing-masing variable independent. Koefisien ini diperoleh dengan cara memprediksi nilai variable dependen dengan suatu persamaan. Koefisien regresi dihitung dengan dua tujuan sekaligus : Pertama, meminimumkan penyimpangan antara nilai actual dan nilai estimasi variable dependen; Kedua, mengoptimalkan korelasi antara nilai actual dan nilai estimasi variable dependen berdasarkan data yang ada. Teknik estimasi variable dependen yang melandasi analisis regresi disebut Ordinary Least Squares (pangkat kuadrat terkecil biasa). 1. REGRESI LINEAR SEDERHANA Persamaan di atas adalah rumus dari persamaan regresi linear sederhana. Y adalah variabel tak bebas, a adalah koefisien intersep, b adalah kemiringan dan t adalah variabel bebas. Rumus untuk b adalah : Dan rumus untuk mendapatkan nilai a adalah sebagai berikut : Dalam regresi linear sederhana juga ada yang disebut dengan koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel yang lain. Rumus untuk menghitung koefisien korelasi adalah sebagai berikut : 2. REGRESI LINEAR BERGANDA Analisis regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. \u200b Persamaan regresi linear berganda sebagai berikut: Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan) Contoh kasus: Kita mengambil contoh kasus pada uji normalitas, yaitu sebagai berikut: Seorang mahasiswa bernama Bambang melakukan penelitian tentang faktor-faktor yang mempengaruhi harga saham pada perusahaan di BEJ. Bambang dalam penelitiannya ingin mengetahui hubungan antara rasio keuangan PER dan ROI terhadap harga saham. Dengan ini Bambang menganalisis dengan bantuan program SPSS dengan alat analisis regresi linear berganda. Dari uraian di atas maka didapat variabel dependen (Y) adalah harga saham, sedangkan variabel independen (X1 dan X2) adalah PER dan ROI. Data-data yang di dapat berupa data rasio dan ditabulasikan sebagai berikut: \u200b \u200b Tabel. Tabulasi Data (Data Fiktif) Tahun Harga Saham (Rp) PER (%) ROI (%) 1990 8300 4.90 6.47 1991 7500 3.28 3.14 1992 8950 5.05 5.00 1993 8250 4.00 4.75 1994 9000 5.97 6.23 1995 8750 4.24 6.03 1996 10000 8.00 8.75 1997 8200 7.45 7.72 1998 8300 7.47 8.00 1999 10900 12.68 10.40 2000 12800 14.45 12.42 2001 9450 10.50 8.62 2002 13000 17.24 12.07 2003 8000 15.56 5.83 2004 6500 10.85 5.20 2005 9000 16.56 8.53 2006 7600 13.24 7.37 2007 10200 16.98 9.38 Langkah-langkah pada program SPSS \u00d8 Masuk program SPSS \u00d8 Klik variable view pada SPSS data editor \u00d8 Pada kolom Name ketik y, kolom Name pada baris kedua ketik x1, kemudian untuk baris kedua ketik x2. \u00d8 Pada kolom Label, untuk kolom pada baris pertama ketik Harga Saham, untuk kolom pada baris kedua ketik PER, kemudian pada baris ketiga ketik ROI. \u00d8 Untuk kolom-kolom lainnya boleh dihiraukan (isian default) \u00d8 Buka data view pada SPSS data editor, maka didapat kolom variabel y, x1, dan x2. \u00d8 Ketikkan data sesuai dengan variabelnya \u00d8 Klik Analyze - Regression - Linear \u00d8 Klik variabel Harga Saham dan masukkan ke kotak Dependent, kemudian klik variabel PER dan ROI kemudian masukkan ke kotak Independent. \u00d8 Klik Statistics, klik Casewise diagnostics, klik All cases. Klik Continue \u00d8 Klik OK, maka hasil output yang didapat pada kolom Coefficients dan Casewise diagnostics adalah sebagai berikut: \u200b Tabel. Hasil Analisis Regresi Linear Berganda Persamaan regresinya sebagai berikut: Y\u2019 = a + b1X1+ b2X2 Y\u2019 = 4662,491 + (-74,482)X1 + 692,107X2 Y\u2019 = 4662,491 - 74,482X1 + 692,107X2 Keterangan: Y\u2019 = Harga saham yang diprediksi (Rp) a = konstanta b1,b2 = koefisien regresi X1 = PER (%) X2 = ROI (%) Persamaan regresi di atas dapat dijelaskan sebagai berikut: - Konstanta sebesar 4662,491; artinya jika PER (X1) dan ROI (X2) nilainya adalah 0, maka harga saham (Y\u2019) nilainya adalah Rp.4662,491. - Koefisien regresi variabel PER (X1) sebesar -74,482; artinya jika variabel independen lain nilainya tetap dan PER mengalami kenaikan 1%, maka harga saham (Y\u2019) akan mengalami penurunan sebesar Rp.74,482. Koefisien bernilai negatif artinya terjadi hubungan negatif antara PER dengan harga saham, semakin naik PER maka semakin turun harga saham. - Koefisien regresi variabel ROI (X2) sebesar 692,107; artinya jika variabel independen lain nilainya tetap dan ROI mengalami kenaikan 1%, maka harga saham (Y\u2019) akan mengalami peningkatan sebesar Rp.692,107. Koefisien bernilai positif artinya terjadi hubungan positif antara ROI dengan harga saham, semakin naik ROI maka semakin meningkat harga saham. Nilai harga saham yang diprediksi (Y\u2019) dapat dilihat pada tabel Casewise Diagnostics (kolom Predicted Value). Sedangkan Residual ( unstandardized residual ) adalah selisih antara harga saham dengan Predicted Value, dan Std. Residual ( standardized residual ) adalah nilai residual yang telah terstandarisasi (nilai semakin mendekati 0 maka model regresi semakin baik dalam melakukan prediksi, sebaliknya semakin menjauhi 0 atau lebih dari 1 atau -1 maka semakin tidak baik model regresi dalam melakukan prediksi).","title":"Regresi Linear"},{"location":"regresi/#regresi-linear-sederhana-dan-regresi-linear-berganda","text":"PENGERTIAN Regresi secara umum adalah studi mengenai ketergantungan suatu variabel dependen (terikat) dengan satu atau lebih variabel independent (variabel penjelas/bebas). Tujuannya adalah untuk mengestimasi dan/atau memprediksi rata-rata populasi atau niiai rata-rata variabel dependen berdasarkan nilai variabel independen yang diketahui. Pusat perhatian adalah pada upaya menjelaskan dan mengevalusihubungan antara suatu variabel dengan satu atau lebih variabel independen. Hasil analisis regresi adalah berupa koefisien regresi untuk masing-masing variable independent. Koefisien ini diperoleh dengan cara memprediksi nilai variable dependen dengan suatu persamaan. Koefisien regresi dihitung dengan dua tujuan sekaligus : Pertama, meminimumkan penyimpangan antara nilai actual dan nilai estimasi variable dependen; Kedua, mengoptimalkan korelasi antara nilai actual dan nilai estimasi variable dependen berdasarkan data yang ada. Teknik estimasi variable dependen yang melandasi analisis regresi disebut Ordinary Least Squares (pangkat kuadrat terkecil biasa). 1. REGRESI LINEAR SEDERHANA Persamaan di atas adalah rumus dari persamaan regresi linear sederhana. Y adalah variabel tak bebas, a adalah koefisien intersep, b adalah kemiringan dan t adalah variabel bebas. Rumus untuk b adalah : Dan rumus untuk mendapatkan nilai a adalah sebagai berikut : Dalam regresi linear sederhana juga ada yang disebut dengan koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel yang lain. Rumus untuk menghitung koefisien korelasi adalah sebagai berikut : 2. REGRESI LINEAR BERGANDA Analisis regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. \u200b Persamaan regresi linear berganda sebagai berikut: Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan) Contoh kasus: Kita mengambil contoh kasus pada uji normalitas, yaitu sebagai berikut: Seorang mahasiswa bernama Bambang melakukan penelitian tentang faktor-faktor yang mempengaruhi harga saham pada perusahaan di BEJ. Bambang dalam penelitiannya ingin mengetahui hubungan antara rasio keuangan PER dan ROI terhadap harga saham. Dengan ini Bambang menganalisis dengan bantuan program SPSS dengan alat analisis regresi linear berganda. Dari uraian di atas maka didapat variabel dependen (Y) adalah harga saham, sedangkan variabel independen (X1 dan X2) adalah PER dan ROI. Data-data yang di dapat berupa data rasio dan ditabulasikan sebagai berikut: \u200b \u200b Tabel. Tabulasi Data (Data Fiktif) Tahun Harga Saham (Rp) PER (%) ROI (%) 1990 8300 4.90 6.47 1991 7500 3.28 3.14 1992 8950 5.05 5.00 1993 8250 4.00 4.75 1994 9000 5.97 6.23 1995 8750 4.24 6.03 1996 10000 8.00 8.75 1997 8200 7.45 7.72 1998 8300 7.47 8.00 1999 10900 12.68 10.40 2000 12800 14.45 12.42 2001 9450 10.50 8.62 2002 13000 17.24 12.07 2003 8000 15.56 5.83 2004 6500 10.85 5.20 2005 9000 16.56 8.53 2006 7600 13.24 7.37 2007 10200 16.98 9.38 Langkah-langkah pada program SPSS \u00d8 Masuk program SPSS \u00d8 Klik variable view pada SPSS data editor \u00d8 Pada kolom Name ketik y, kolom Name pada baris kedua ketik x1, kemudian untuk baris kedua ketik x2. \u00d8 Pada kolom Label, untuk kolom pada baris pertama ketik Harga Saham, untuk kolom pada baris kedua ketik PER, kemudian pada baris ketiga ketik ROI. \u00d8 Untuk kolom-kolom lainnya boleh dihiraukan (isian default) \u00d8 Buka data view pada SPSS data editor, maka didapat kolom variabel y, x1, dan x2. \u00d8 Ketikkan data sesuai dengan variabelnya \u00d8 Klik Analyze - Regression - Linear \u00d8 Klik variabel Harga Saham dan masukkan ke kotak Dependent, kemudian klik variabel PER dan ROI kemudian masukkan ke kotak Independent. \u00d8 Klik Statistics, klik Casewise diagnostics, klik All cases. Klik Continue \u00d8 Klik OK, maka hasil output yang didapat pada kolom Coefficients dan Casewise diagnostics adalah sebagai berikut: \u200b Tabel. Hasil Analisis Regresi Linear Berganda Persamaan regresinya sebagai berikut: Y\u2019 = a + b1X1+ b2X2 Y\u2019 = 4662,491 + (-74,482)X1 + 692,107X2 Y\u2019 = 4662,491 - 74,482X1 + 692,107X2 Keterangan: Y\u2019 = Harga saham yang diprediksi (Rp) a = konstanta b1,b2 = koefisien regresi X1 = PER (%) X2 = ROI (%) Persamaan regresi di atas dapat dijelaskan sebagai berikut: - Konstanta sebesar 4662,491; artinya jika PER (X1) dan ROI (X2) nilainya adalah 0, maka harga saham (Y\u2019) nilainya adalah Rp.4662,491. - Koefisien regresi variabel PER (X1) sebesar -74,482; artinya jika variabel independen lain nilainya tetap dan PER mengalami kenaikan 1%, maka harga saham (Y\u2019) akan mengalami penurunan sebesar Rp.74,482. Koefisien bernilai negatif artinya terjadi hubungan negatif antara PER dengan harga saham, semakin naik PER maka semakin turun harga saham. - Koefisien regresi variabel ROI (X2) sebesar 692,107; artinya jika variabel independen lain nilainya tetap dan ROI mengalami kenaikan 1%, maka harga saham (Y\u2019) akan mengalami peningkatan sebesar Rp.692,107. Koefisien bernilai positif artinya terjadi hubungan positif antara ROI dengan harga saham, semakin naik ROI maka semakin meningkat harga saham. Nilai harga saham yang diprediksi (Y\u2019) dapat dilihat pada tabel Casewise Diagnostics (kolom Predicted Value). Sedangkan Residual ( unstandardized residual ) adalah selisih antara harga saham dengan Predicted Value, dan Std. Residual ( standardized residual ) adalah nilai residual yang telah terstandarisasi (nilai semakin mendekati 0 maka model regresi semakin baik dalam melakukan prediksi, sebaliknya semakin menjauhi 0 atau lebih dari 1 atau -1 maka semakin tidak baik model regresi dalam melakukan prediksi).","title":"Regresi Linear Sederhana dan Regresi Linear Berganda"}]}